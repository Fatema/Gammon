\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\graphicspath{}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Reinforcement Learning, Looking for New Backgammon Strategies}
\author{Fatema Alkhanaizi}
\student{Fatema Alkhanaizi}
\supervisor{Rob Powell}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\subsection{Context/Background}
Backgammon has been the subject of many studies involving artificial intelligence and machine learning. 
TD-Gammon  of Tesauro \cite{} had demonstrated the impressive ability of machine learning techniques to learn to play games. TD-Gammon used reinforcement learning techniques with a Neural Network (NN) that trains itself to be an evaluation function for the game of backgammon, by playing against itself and learning from the outcome \cite{}. However, the monolithic nueral network soon reached its limitation an outcome studied by Boyan \cite{} and a modular nueral network becomes more suitable to over come this limitation. The newest software for Backgammon build on top of the modular architecture such as eXtreme Gammon \cite{} and GNUBG \cite{}.
\subsection{Aims}
The aim of this project is to find new startegy for backgammon; a hybrid of known startegies will be used as the basis for the new startegy. 
\subsection{Method}
A modular neural network architecture will be used to incorporate the different backgammon strategies. The priming and back games will be used for this project. Two modular networks will be implemented and trained, one that will include the 2 strategies seperately and another one that will include a hybrid of the 2 startegies. A single nueral network based on TD Gammon will also be implemented and trained. The modular networks will be evaulated against the single network. Test games against an expert user will be included to validate the new startegy.
\subsection{Proposed Solution}
A python script that allows a user 

\end{abstract}
\begin{keywords}
Backgammon; Reinforcement Learning; Modular Nueral Network; 
\end{keywords}

\section{Introduction}
\subsection{Backgammon}
Backgammon is a game played with dice and checkers on a board consisting of 24 fields, in which each player tries to move his checkers home and bear them off while preventing the opponent from doing the same thing \cite{}. Figure \ref{board} illustrates the basic setup that will be used for this project.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{boardsetup}.png}
    \end{minipage}
    \caption{Backgammon board setup}
    \label{bbs}
\end{figure}

\subsection{Early work}
Backgammon and many other boardgames have been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. TD-Gammon  of Tesauro \cite{} had demonstrated the impressive ability of machine learning techniques to learn to play games. TD-Gammon used reinforcement learning techniques with a single neural network that trains itself to be an evaluation function for the game of backgammon, by playing against itself and learning from the outcome \cite{}. Extending on the work of Tesauro, Boyan \cite{} showed that the use of different networks for different subtasks of the game can improve over learning with a single neural network for the entire game. Many AI softwares for Backgammon use modular neural network architecture; GNU-Backgammon \cite{} uses 3 different nueral networks. 

\subsection{Project Overview}
This project will focus on modular neural network architecture in incoporating a hybrid of backgammon startegies to find a new strategy. In addition, the performance of the new strategy will be evaluated by comparing it to another network that includes the strategies seperately. Combinations of 2 startegies will be studied: \textbf{The back game}, the player tries to hold both anchors as long as possible and force his opponent to bear in or bear off awkwardly \cite{}. \textbf{The priming game}, a particular type of holding game that involves building a prime — a long wall of the player's pieces, ideally 6 points in a row — in order to block the movement of the opponent’s pieces that are behind the wall \cite{}.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime}.png}
    \end{minipage}
    \hspace{1.2em}
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{backgame}.png}
    \end{minipage}
    \caption{Illustration of Prime and Back games}
    \label{games}
\end{figure}

\subsection{Rsearch Questions}
The following questions will be investigated in this project:
\begin{enumerate}
    \item How will combining game strategies for backgammon influence learning outcome? 
    \item Would using hybrid startegies result in the agent learning a new strategy? or would one strategy be more dominant than the other?
    \item How effective would the hybrid startegies if the startegies were included seperately?
    \item How would adding doubling cube to the network evaluation influence the learning outcome?
\end{enumerate}
\subsection{Deliverables}
\begin{itemize}
    \item minimum objectives
    \begin{enumerate}
    \item create a basic backgammon player to be used as a bench player 
    \item create a player that uses the Priming Game 
    \item create a player that uses the Back Game
    \item create a player that combines the Priming and Back Games
    \item evaluate the performance of the players with strategies against the basic player
    \end{enumerate}
    \item intermediate objectives
    \begin{enumerate}
    \item create a basic player with a bigger hidden network  
    \item evaluate the performance of the best player with a strategy against the new basic player
    \end{enumerate}
    \item advanced objectives
    \begin{enumerate}
    \item include doubling cube to the players  
    \item perform a rollout for one of the games
    \item test the effect of including other Backgammon Game strategies
    \end{enumerate}
    \end{itemize}
\section{Design}

\subsection{Requirements}
\begin{table}[htb]
    \centering
    \caption{List of Functional Requirements}
    \vspace*{6pt}
    \label{req}
    \begin{tabular}{cp{12cm}c}
        \hline
        \hline
        ID & Requirement & Priority \\ 
        \hline
        FR1 & A module for Backgammon game must be implemented. This will include the actual board setup with the rules and constraints of the game e.g. legal moves and the dice role & High \\
        \hline
        FR2 & An AI agent should be created such that it uses a nueral network to evaulate legal moves/actions to play backgammon and a greedy search algorithm to pick the best legal move/action & High \\
        \hline
        FR3 & A module for the nueral network should be created. It should support the funtionalities required for the nueral network e.g. updating weights through back-propagation, saving and restoring the network meta-data & High \\
        \hline
        FR3 & A module for the training and testing the nueral network should be created & High \\
        \hline
        FR3 & Monolithic Nueral Network should be implemented and trained based on Tesauro's TD Gammon & High \\
        \hline
        FR7 & Modular Nueral Network that includes Holding and Priming Game strategies seperately should be implemented and trained & High \\
        \hline
        FR7 & Modular Nueral Network that includes a hybrid of Holding and Priming Game strategies should be implemented and trained & High \\
        \hline
        FR7 & A testing module should provide capabilities of evaluating and testing the nueral networks & High \\
        \hline
        FR8 & depth (n-ply) search algorithm should be implemented and incoporated into AI agent in FR2 & Medium \\
        \hline
        FR9 & Textual User interface for a Human agent to play against the AI agent from FR2 should be implemented & Medium \\
        \hline
        FR10 & Graphical User interface for a Human agent to play against the AI agent from FR2 should be implemented & Low \\
        \hline
        FR10 & Doubling cude evaluation. A hueristic function should be implemented to determine when to use the doubling cube or to accept or refuse the double & Low \\
        \hline
        
    \end{tabular}
\end{table}
\begin{table}[htb]
    \centering
    \caption{List of Non-functional Requirements}
    \vspace*{6pt}
    \label{nonreq}
    \begin{tabular}{cp{12cm}c}
        \hline
        \hline
        ID & Requirement & Priority \\ 
        \hline
        NFR1 & Trained networks should return the outcome from forward propagation within 40ms & Medium \\
        \hline
        NFR2 & The AI agent should pick a legal move/action within 1s. In other word the search algorithm for the best move/action should return a value within 1s & Medium \\
        \hline
        NFR3 & The AI agent should pick a legal move 100\% of the time & High \\
        \hline
    \end{tabular}
\end{table}
\subsection{Algorithms}
\subsubsection{Temporal Difference, TD($\lambda$)}
Temporal difference will be used 

\subsubsection{1-ply search algorithm}
A ply is one turn taken by one user; n-ply refers to how far the player will look ahead when evaluating a move/action \cite{}. For the first stages of the project, the AI agent will use a 1-ply search algorithm to pick the best legal action for the current turn. Each action will be evaulated in the nueral network, forward propagation, and the action with the maximum outcome will be returned.

\subsection{System Components}
Python 3.6 will used as the language for this project. The nueral networks will be implemented using tensorflow package. Tensorflow will be used as it is easy to use, to generate training progress summary, to save and to restore the trained network. Figure \ref{syscomp} shows the expected structure and component dependencie of this project. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{syscomp}.png}
    \end{minipage}
    \caption{System components and dependencies}
    \label{syscomp}
\end{figure}

\subsubsection{Game}
This module will hold the game setup and define the rules and constraints of the game e.g. take an action, find legal moves and game board setup. An open source implementation taken from Awni github repository of this module will be refactored and modified for the use of this project. 

\subsubsection{Agents}
There are 2 types of agents that will be implemented for this project: 
\begin{itemize}
    \item \textbf{A human agent}, an interactive agent which will take user inputs either from the command line or by capturing the user clicks though a GUI to make a move/action.
    \item \textbf{AI agent} will use a modular nueral network to determine the move/action for the current turn. A list of legal moves is obtained from the game module and an action will be picked based on the search algorithm.
\end{itemize}

\subsubsection{Modnet}
This module will define the operations for extracting features from the game board, testing and training nueral network/s. This module will heavily depend on Subnet module. For modular networks, a game-specific gating program will be implemented in this module to determine which sub-network will be suitable to a given input, set of extracted features. For the different modular nueral networks to be trained for this project, different instances of this module will be created as each modular network will require different gating program. The monolithic nueral network won't need the gating program.

\subsubsection{Subnet}
This module will include the Nueral Network implementation using tensorflow. It will provide routines for storing and accessing model, checkpoints and summaries generated by tensorflow. In addition, it will include the forward propagation and backpropagation algorithms. All networks created for this project will use an instance of this module; networks used in modular nueral network and monolithic nueral network. The architecture of these networks will be explained in the next section.

\subsection{Nueral Network Architecture} \label{modnet}
\subsubsection{Monolithic Nueral Network}
For this network, it will be based on Tesauro's TD Gammon implementation (Tesauro 1992, 2002); a fully-connected feed-forward nueral networks with a single hidden layer. Initially, the architecture will consist of one input layer I with 298 units which will consist of 288 raw inputs representing the checkers configuration on the board, each field in the board is represented by 6 units as there are 24 fields so 144 total units and each player has thier own configuration represented seperately making the total 288. In addition, 8 input units will be included as expert features, table-\ref{exfeat}. Those expert features proved to provide better outcome from the network \cite{}. Lastly, 2 input units to represent the current player's token. 
\begin{table}[htb]
    \centering
    \caption{Possible expert features for input layer}
    \vspace*{6pt}
    \label{exfeat}
    \begin{tabular}{cp{12cm}}
        \hline
        \hline
        Feature name & Description \\ 
        \hline
        bar\_pieces\_1 & number of checkers held on the bar for current player\\
        \hline
        bar\_pieces\_2 & number of checkers held on the bar for opponent\\
        \hline
        pip\_count\_1 & pip count for current player \\
        \hline
        pip\_count\_2 & pip count for opponent \\
        \hline
        off\_pieces\_1 & percentage of pieces that current player has beared off \\
        \hline
        off\_pieces\_2 & percentage of pieces that opponent has beared off \\
        \hline
        hit\_pieces\_1 & percentage of pieces that are at risk of being hit (single checker in a position which the opponent can hit) for current player \\
        \hline
        hit\_pieces\_2 & percentage of pieces that are at risk of being hit (single checker in a position which the player can hit) for opponent\\
        \hline
    \end{tabular}
\end{table}

As part of the network architecture, there will be one hidden layer H with 50 units and one output layer O with 1 unit representing the winning probability. The network will have weights $w_{ih}$ for all input units $I_i$ to hidden unit $H_h$ and weights $w_{ho}$ for all hidden units $H_h$ to output unit $O_o$. The weights will be intialized to be random values, hence the initial strategy is a random strategy. Each hidden unit and output unit will have a bias $b_h$ and $b_o$ with sigmoid activation. Each bias will be intialized to an array of constant values of $0.1$. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mononn}.png}
    \end{minipage}
    \caption{Nueral Network architecture}
    \label{board}
\end{figure}

 A lot of implementations of this network are available in the open source community and will be used as a reference for this project; two code basis from github \_ and \_ will be used and referred through out the life cycle of this project. The main challenge with using open source code will be debugging the code and validating it.


\subsubsection{Modular Nueral Network}
To incorporate backgammon startegies, modular nueral architecture will be implemented. The strategies will be represeneted by different monolithic nueral networks that will be activated when certain board configurations are reached. This approach has been implemented by Boyan() and what most recent softwares such as GNU-Backgammpn follow. The modular networks that will be implemented for this project will consist of a combination of the following networks:
\begin{enumerate}
    \item One network for racing game; the checkers cannot be hit anymore by another checker or the checkers layout is close to this outcome.
    \item One network for back game positions; the player is behind in the race (pipcount) but has two or more anchors (two checkers at one field) in the opponent's home board. This network will also be used when there are many checkers on the bar.
    \item One network for priming games; if the player has a prime of 4-5 fields (a long wall of checkers) 
    \item One network for a hybrid priming and back game; combines the conditions for both games
    \item One default network for all other positions
\end{enumerate}
Those networks will be used to allow the learning agent to better evaluate the positions and reach a strategy that is close to the strategy followed by professional players from each network. Initially each network will have the same layout/architecture as the monolithic nueral network, however the networks don't necessarly need to have the same layout. At later stages, different layouts will be tested to configure each nueral network e.g. racing game network does not need inputs for all fields since most checkers will be beared off or close to being beared off. This could help reduce computing time and thus increase the efficiency of the training.

There are two types of Modular Nueral Network architectures that will be implemented in this project:
\begin{itemize}
    \item \textbf{Designer Domain Decomposition Network (DDD) -}This architecture will be used in the first stages of the project. The DDD network consists of n monolithic nueral networks and a hard-coded gating program, figrue \ref{ddd}. The gating program, based on the work of Boyan (year), partition the domain space into n classes. For this project the n classes are represented by the different backgammon strategies; there are other possible decompositions for the backgammon strategies but Racing, Back and Prime games will be the focus for this project. 
    \begin{figure}[htb] 
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{{dddnn}.png}
        \end{minipage}
        \caption{The DDD Network}
        \label{ddd}
    \end{figure}
    In both forward feeding and backward propagation, the gating program will be called to select an appropiate network for the current board extracted features (inputs). Exactly one network will be activate at any time.

    \item \textbf{Meta-Pi Networks -}
    The gating program in the DDD network will suffer from a blemish effect as noted by Boyan (year). The Meta-Pi network is a trainable gating network, figure \ref{metapi}.
    \begin{figure}[htb] 
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{{metapinn}.png}
        \end{minipage}
        \caption{Meta-Pi gating network}
        \label{metapi}
    \end{figure}
    This network will be used to determine the most suited network to be triggered based on a give input. The benefit of this approach is that it will reduce the stiffness introduced by hard coding the triggers for the networks and will allow the agent to develop a smoother evaluation function. This network will require the other networks to be fully trained and only the meta-pi network would be updated in the training process. Thus, this network will be introduced in later stages of the project once all networks have been trained.
\end{itemize}

\subsection{Training}
A total of 5 networks will be trained; 1 monolithic network and 4 modular network with different strategy combinations. 

The training strategy will be based on the work of Tesauro, Boyan and Weiring. The monolithic network and the modular networks with the DDD architecture will be trained by self-play using TD($\lambda$) learning with a decaying learning rate $\alpha$ strating from 0.1 until 0.01, a discount factor $\gamma$ of 1 and a decaying value for $\lambda$ starting from 0.9 until 0.7; exponetial decay will be used for both learning rate $\alpha$ and $\lambda$. Each network will be trained on 500,000 games. After each 5000 games, the network will be tested for 2500 games against the previously stored version of the network itself. This will allow to monitor the progress of the network's training. 

Before running the full training with 500,000 games, different configurations for each network will be tested e.g. the addition of expert features as part of the input layer. The number of training games will be reduced to 100,000 and the resultant network will be tested against the other configurations of that network for 2000 games. The best combination of configurations will be used to fully train each network.

Once DDD networks are finished training, meta-pi gating network will be trained following the same strategy, but it will be continuously tested against the DDD network .

\subsection{Evaluation}
All modular networks will be tested for 5000 games against the monolithic network. The result obtained will give an indication of the general performance of the modular networks and effictiveness of the implemented architecture. 

Test games with certain starting configurations, 5 different configurations for each racing, back and prime games, will also be used to evaluate the strategies followed by each modular network, answering the question if the modular networks have indeed learned the strategy imposed for the specific network. The dice roll for each game will be set such that all networks will have the same dice roll each turn. The opponent will be set to be an ai agent that use the monolithic network. 1000 games will be ran for each configuration. For a random sample of games, the actions taken by the AI agent will be recorded and manually traced for each network. Other statistical values will also be collected and averaged such as the number of turns taken and the end outcome (win/lose). 

To further evaluate the networks, few test games against an expert-level backgammon human player will be conducted. The expert will be asked to provide feedback about the AI agent's actions and strategy for each modular network. The expert won't be told any details regarding the model that the AI agent will be using. 

The last evaluation will be for the best trained network in which 5000 games for each modular network against the other nueral networks will be tested. 

\subsection{Extensions}
\subsubsection{Doubling Cube}
As an extension of this project, doubling cube will be included as part of evaulating the actions taken by the AI agent. This will be implemented by including a hueristic function as part of the action evaluation process. The best trained network will be retrained with the doubling cube taken into consideration. The newly trained network will be tested for 5000 games against the older version of the network. 

\subsubsection{2-ply search algorithm}
% (https://www.gnu.org/software/gnubg/manual/html_node/The-depth-to-search-and-plies.html)
Following Tesauro's research and recent backgammon softwares, the search algorithm used to determine the current turn's move will effect the general training outcome; it is evident from (backgammon league table) that better results are expected from 2-ply and 3-ply search. The 2-ply algorithm algorithm will be implemented as follows:

\begin{algorithm}
    \caption{2-ply search}\label{euclid}
    \begin{algorithmic}[1]
    \State $legalActions \gets \textit{getLegalActions()}$
    \State $actions \gets []$
    \For {$action \textit{ in } legalActions$}
    \State $\textit{takeAction}(action)$
    \State $features \gets \textit{extractFeatures}()$
    \State $v \gets \textit{getModelOutput}(features)$ \Comment{get the outcome of the taken action, forward propagation}
    \State $actions.\textit{append}((action,v))$
    \State $\textit{undoAction}(action)$
    \EndFor
    \State $\textit{sortActions}(actions)$ \Comment{sort actions in descending order}
    \State $topActions \gets \textit{getSubList}(actions, 5)$ \Comment{get first five actions}
    \State $actionsOutcome \gets []$
    \For {$action \textit{ in } topActions$}
    \State $\textit{takeAction}(action)$
    \State $outcomes \gets \textit{runAllPossibleDiceRollForOpp}()$ \Comment{20 legal moves considered for each roll, the best outcome of the move of each roll will be returned}
    \State $avgOutcome \gets \textit{avg}(outcomes)$
    \State $actionsOutcome.\textit{append}((action, avgOutcome))$
    \State $\textit{undoAction}(action)$
    \EndFor
    \Return $\textit{getBestRankAction}(actionsOutcome)$ \Comment{best rank action }
    \end{algorithmic}
    \end{algorithm}

It is important to note that this computation could take some time and in timed games this could be unfavourable. The only difference between 1-ply and 2-ply is that the 1-ply won't check all the possible rolls of the opponents and stops after evaulating the immediate best action.

\bibliography{projectpaper}


\end{document}