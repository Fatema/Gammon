\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\graphicspath{}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Reinforcement Learning, Looking for New Backgammon Strategies}
\author{Fatema Alkhanaizi}
\student{Fatema Alkhanaizi}
\supervisor{Rob Powell}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\subsection{Context/Background}
Backgammon and many other boardgames have been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Using Reinforcement Learning techniques to play backgammon has given insight to potential strategies that were overlooked in the past.
\subsection{Aims}
The aim of this project is to find a new startegy for backgammon; a hybrid of known startegies will be used as the basis for the new startegy. 
\subsection{Method}
A modular neural network architecture will be used to incorporate the different backgammon strategies. The priming and back games will be used for this project. Two modular networks will be implemented and trained, one that will include the 2 strategies seperately and another one that will include a hybrid of the 2 startegies. A single nueral network based on TD Gammon will also be implemented and trained. The modular networks will be evaulated against the single network and against each other. Test games against an expert user will be included to validate the new startegy.
\subsection{Proposed Solution}
A python package that will include modules to train and test the networks using self-play. It will also include a module for setting both a textual and a graphical user interfaces to play against the trained networks. 
\end{abstract}

\begin{keywords}
Backgammon; Reinforcement Learning; Modular Nueral Network; Priming Game; Back Game;
\end{keywords}

\section{Introduction}
\subsection{Backgammon}
Backgammon is a game played with dice and checkers on a board consisting of 24 fields, in which each player tries to move his checkers home and bear them off while preventing the opponent from doing the same thing \cite{glossary}. Figure \ref{bbs} illustrates the basic setup that will be used for this project.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{boardsetup}.png}
    \end{minipage}
    \caption{Backgammon board setup}
    \label{bbs}
\end{figure}

\subsection{Reinforcement Learning}
A reinforcement learning algorithms allows an agent to learn from its experience generated by its interactions with an environment \cite{rl}. Temporal difference (TD) learning is a class of reiforcement learning which uses the difference between two successive positions for backpropagating the evaluations of the successive positions to the current position. There is a whole family of temporal difference algorithms known as TD($\lambda$)-algorithms which are parametized by the value $\lambda$ which makes the agent looks further in the future for updating its value function \cite{rl}. 

\subsection{Early work}
TD-Gammon  of Tesauro \citeyear{DBLP:conf/icml/Tesauro92} used TD($\lambda$) reinforcement learning methods with a single neural network that trains itself and learns from the outcomes to be an evaluation function for the game of backgammon. Extending on the work of Tesauro, Boyan \citeyear{boyan} showed that the use of different networks for different subtasks of the game can perform better than learning with a single neural network for the entire game. Many AI softwares for Backgammon use similar modular neural network architecture; GNU-Backgammon uses 3 different nueral networks for their evaluation function \cite{gnubg}.

\subsection{Project Overview}
This project will focus on using modular neural network architecture to incoporate a hybrid of backgammon startegies to find a new game strategy. In addition, the performance of the new strategy will be evaluated by comparing it to another network that includes the strategies seperately. Combinations of 2 startegies will be studied: \textbf{The back game}, the player tries to hold two or more anchors, points occupied by two or more checkers in the opponent's home board, as long as possible and force the opponent to bear in or bear off awkwardly \cite{glossary}. \textbf{The priming game}, a particular type of holding game that involves building a prime — a long wall of the player's pieces, ideally 6 points in a row — in order to block the movement of the opponent’s pieces that are behind the wall \cite{glossary}.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime}.png}
    \end{minipage}
    \hspace{1.2em}
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{backgame}.png}
    \end{minipage}
    \caption{Illustration of Priming and Back games}
    \label{games}
\end{figure}

\subsection{Rsearch Questions}
The following questions will be investigated in this project:
\begin{enumerate}
    \item Would using hybrid startegies result in the agent learning a new strategy? or would one strategy be more dominant than the other?
    \item How effective would the hybrid startegies be over the startegies being included seperately?
    \item How would adding doubling cube to the network evaluation influence the learning outcome?
\end{enumerate}
\subsection{Deliverables}
\begin{itemize}
    \item \textbf{Trained neural networks}: A single neural network based on Tesauro's TD Gammon, a modular neural network that uses a new startegy, a hybrid priming and back startegy, and a modular neural network that uses two seperate known strategies, priming and back strategies.
    \item \textbf{AI agent}: This agent will be able to use the trained network to evaluate and make moves.
    \item \textbf{User interface and human agent}: This will be a simple command line interface which takes user inputs to make moves and to play against the trained networks. Another complex implementation will be a graphical user interface which captures the user clicks for making moves.
    \item \textbf{Testing suit}: This will be used to run all tests and evaluations to be done on the networks.
    \item \textbf{Project Report}: All tests and evaluations will be recorded and analysed in this report.
\end{itemize}

\pagebreak

\section{Design}
\subsection{Requirements}
Table \ref{req} includes the functional requirements for this project.
\begin{table}[htb]
    \centering
    \caption{List of Functional Requirements}
    \vspace*{6pt}
    \label{req}
    \begin{tabular}{cp{12cm}c}
        \hline
        \hline
        ID & Requirement & Priority \\ 
        \hline
        FR1 & A module for Backgammon game must be implemented. This will include the actual board setup with the rules and constraints of the game e.g. legal moves and the dice role & High \\
        \hline
        FR2 & An AI agent should be created such that it uses a nueral network to evaulate legal moves/actions to play backgammon and uses 1-ply search algorithm to pick the best legal move/action & High \\
        \hline
        FR3 & A module for the nueral network should be created. It should support the funtionalities required for the nueral network e.g. updating weights through back-propagation, saving, and restoring the network's meta-data & High \\
        \hline
        FR4 & A module for training the nueral network should be created & High \\
        \hline
        FR5 & Single Nueral Network should be implemented and trained based on Tesauro's TD Gammon, extends FR4 AND FR3 & High \\
        \hline
        FR6 & Modular Nueral Network that includes Holding and Priming Game strategies seperately should be implemented and trained, extends FR4 AND FR3 & High \\
        \hline
        FR7 & Modular Nueral Network that includes a hybrid of Holding and Priming Game strategies should be implemented and trained, extends FR4 AND FR3 & High \\
        \hline
        FR8 & A testing module should provide capabilities of evaluating and testing the nueral networks, used fortesting FR5 to FR7 & High \\
        \hline
        FR9 & 2-ply search algorithm should be implemented and incoporated into AI agent in FR2 & Medium \\
        \hline
        FR10 & Textual User interface for a Human agent to play against the AI agent from FR2 should be implemented & Medium \\
        \hline
        FR11 & Graphical User interface for a Human agent to play against the AI agent from FR2 should be implemented, extending on FR10 & Low \\
        \hline
        FR12 & Doubling cude evaluation. A hueristic function should be implemented to determine when to use the doubling cube or to accept or refuse the double. This should be included to FR4 & Low \\
        \hline
    \end{tabular}
\end{table}

Table \ref{nonreq} includes the non-functional requirements for this project.
\begin{table}[htb]
    \centering
    \caption{List of Non-functional Requirements}
    \vspace*{6pt}
    \label{nonreq}
    \begin{tabular}{cp{12cm}c}
        \hline
        \hline
        ID & Requirement & Priority \\ 
        \hline
        NFR1 & Trained networks should return the outcome from forward propagation within 40ms & Medium \\
        \hline
        NFR2 & The AI agent should pick a legal move/action within 1s. In other word the search algorithm for the best move/action should return a value within 1s & Medium \\
        \hline
        NFR3 & The AI agent should pick a legal move 100\% of the time & High \\
        \hline
    \end{tabular}
\end{table}
\subsection{Algorithms}
\subsubsection{Temporal Difference, TD($\lambda$)}
In backgammon, the dice rolls guarantee sufficient variability in the games so that all regions of the feature space will be explored. This characteristic made it perfectly suited for learning from self-play as it overcomes the known Reinforcement Learning problem with the trade-off between exploration and exploitation \cite{survey}. TD Gammon used the gradient-decent form of the TD($\lambda$) algorithm with the gradients computed by the error backpropogation algorithm \cite{rl}. The update rule is as follows
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha\delta_t\vec{e}_{t}$$ 
where $\delta_t$ is the TD error,
$$\delta_{t} = r_{t+1} + \gamma V_t(s_{t+1}) - V_t(s_t)$$ 
and $\vec{e}_t$ is a column vector of eligibility traces, one for each component of $\vec{\theta}_t$, updated by 
$$\vec{e}_{t} = \gamma\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)$$ 
The expression $\nabla_{\vec{\theta}_{t}}V_t(s_t)$ referes to the gradient. For backgammon, $\gamma=1$ and the reward is always zero except when winning, reducing the update rule to
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha[V_t(s_{t+1}) - V_t(s_t)][\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)]$$
$\alpha$, the learning rate, and $\lambda$ are constraint by the range $(0,1)$. The ideal value of $\lambda$ is between $0.7$ and $0.6$ based on Tesauro's \citeyear{DBLP:journals/ai/Tesauro02} results. $\alpha$ on the other hand, should idealy decay. $e_t(s)$ is the eligibility trace for state $s$, it marks $s$ as eligible for undergoing learning changes when a TD error, $\delta_t$ occurs \cite{rl}.

\subsubsection{1-ply search algorithm}
A ply is one turn taken by one user; n-ply refers to how far the player will look ahead when evaluating a move/action \cite{glossary}. Initially, the AI agent will use a 1-ply search algorithm to pick the best legal action for the current turn. Each action will be evaulated in the nueral network, forward feeding, and the action with the maximum outcome will be returned \cite{DBLP:conf/icml/Tesauro92}.

\subsection{System Components}
Python 3.6 will used as the language for this project. The nueral networks will be implemented using tensorflow package. Tensorflow will be used as it allows to generate training progress summary, to save and to restore a trained network. Figure \ref{syscomp} shows the expected structure and component dependencie of this project. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{syscomp}.png}
    \end{minipage}
    \caption{System components and dependencies}
    \label{syscomp}
\end{figure}

\subsubsection{Main}
This module will be used to invoke other components in the system and handle the general actions required from the system: play, train and test.  

\subsubsection{Game}
This module will hold the game setup and define the rules and constraints of the game e.g. take an action, find legal moves and game board setup. An open source implementation taken from a github repository, backgammon \cite{awni}, of this module will be refactored and modified for the use of this project. 


\subsubsection{User Interface}
This module will be handle generating the command line interface (textual interface) and the graphical user interface. The use of either interface will depends on the availability of pygame, python package; pygame allows generating a graphical user interface in python. Similarly to the Game module, an open source implementation taken from a github repository, backgammon \cite{awni}, of this module will be refactored and modified for the use of this project. 

\subsubsection{Agents}
There are 2 types of agents that will be implemented for this project: 
\begin{itemize}
    \item \textbf{A human agent}, an interactive agent which will take user inputs either from the command line or by capturing the user clicks though a GUI to make a move/action.
    \item \textbf{AI agent} will use a modular nueral network to determine the move/action for the current turn. A list of legal moves is obtained from the game module and an action will be picked based on the search algorithm.
\end{itemize}

\subsubsection{Modnet}
This module will define the operations for extracting features from the game board, testing and training nueral network/s. This module will heavily depend on Subnet module. For modular networks, a game-specific gating program will be implemented in this module to determine which sub-network will be suitable to a given input, set of extracted features. For the different modular nueral networks to be trained for this project, different instances of this module will be created as each modular network will require different gating program. The monolithic nueral network won't need the gating program.

\subsubsection{Subnet}
This module will include the Nueral Network implementation using tensorflow. It will provide routines for storing and accessing model, checkpoints and summaries generated by tensorflow. In addition, it will include the forward feeding and backpropagation algorithms. All networks created for this project will use an instance of this module; networks used in modular nueral network and monolithic nueral network. The architecture of these networks will be explained in the next section. An open source implementation taken from a github repository, td-gammon \cite{fomorians}, will used as the basis for this module. 

\subsubsection{Tester}
This module will include all evaluations and test routines for the nueral networks. 

\subsection{Nueral Network Architecture}
\subsubsection{Monolithic Nueral Network}
For this network, it will be based on Tesauro's \citeyear{DBLP:journals/neco/Tesauro94} TD Gammon implementation; a fully-connected feed-forward nueral networks with a single hidden layer. Initially, the architecture will consist of one input layer I with 298 units which will consist of 288 raw inputs representing the checkers configuration on the board, each field in the board is represented by 6 units as there are 24 fields so 144 total units and each player has thier own configuration represented seperately making the total 288. In addition, 8 input units will be included as expert features, table-\ref{exfeat}. Including expert features proved to provide better outcomes from the network \cite{DBLP:journals/ai/Tesauro02}. Lastly, 2 input units to represent the current player's token. 
\begin{table}[htb]
    \centering
    \caption{Possible expert features for input layer}
    \vspace*{6pt}
    \label{exfeat}
    \begin{tabular}{cp{12cm}}
        \hline
        \hline
        Feature name & Description \\ 
        \hline
        bar\_pieces\_1 & number of checkers held on the bar for current player\\
        \hline
        bar\_pieces\_2 & number of checkers held on the bar for opponent\\
        \hline
        pip\_count\_1 & pip count for current player \\
        \hline
        pip\_count\_2 & pip count for opponent \\
        \hline
        off\_pieces\_1 & percentage of pieces that current player has beared off \\
        \hline
        off\_pieces\_2 & percentage of pieces that opponent has beared off \\
        \hline
        hit\_pieces\_1 & percentage of pieces that are at risk of being hit (single checker in a position which the opponent can hit) for current player \\
        \hline
        hit\_pieces\_2 & percentage of pieces that are at risk of being hit (single checker in a position which the player can hit) for opponent\\
        \hline
    \end{tabular}
\end{table}
As part of the network architecture, there will be one hidden layer H with 50 units and one output layer O with 1 unit representing the winning probability. At later stages the output layer will be extended to include 4 units, 2 units for the player winning the game and winning by a gammon and 2 units for the player losing the game and losing by a gammon. The network will have weights $w_{ih}$ for all input units $I_i$ to hidden unit $H_h$ and weights $w_{ho}$ for all hidden units $H_h$ to output unit $O_o$. The weights will be intialized to be random values, hence the initial strategy is a random strategy. Each hidden unit and output unit will have a bias $b_h$ and $b_o$ with sigmoid activation. Each bias will be intialized to an array of constant values of $0.1$. Figure \ref{mononn} includes the layout of the nueral network.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mononn}.png}
    \end{minipage}
    \caption{Nueral Network architecture}
    \label{mononn}
\end{figure}

 A lot of implementations of this network are available in the open source community and will be used as a reference for this project; two code basis from github, backgammon \cite{awni} and td-gammon \cite{fomorians}, will be used and referred through out the life cycle of this project. The main challenge with using open source code will be debugging the code and validating it.

\subsubsection{Modular Nueral Network}
To incorporate backgammon startegies, modular nueral architecture will be implemented. The strategies will be represeneted by different monolithic nueral networks that will be activated when certain board configurations are reached. This approach has been implemented by Boyan \citeyear{boyan} and what most recent softwares such as GNU-Backgammon \cite{gnubg} follow. The modular networks that will be implemented for this project will consist of a combination of the following networks:
\begin{enumerate}
    \item One network for back game positions; the player is behind in the race (pipcount) but has two or more anchors (two checkers at one field) in the opponent's home board. This network will also be used when there are many checkers on the bar.
    \item One network for priming games; if the player has a prime of 4-5 fields (a long wall of checkers) 
    \item One network for a hybrid priming and back game; combines the conditions for both games
    \item One default network for all other positions
\end{enumerate}

Initially each network will have the same layout/architecture as the monolithic nueral network, however the networks don't necessarly need to have the same layout.

There are two types of Modular Nueral Network architectures that will be implemented in this project:
\begin{itemize}
    \item \textbf{Designer Domain Decomposition Network (DDD) -}This architecture will be used in the first stages of the project. The DDD network consists of n monolithic nueral networks and a hard-coded gating program, figrue \ref{ddd}. The gating program, based on the work of Boyan \citeyear{boyan}, partitions the domain space into n classes. For this project the n classes are represented by the different backgammon strategies. 
    \begin{figure}[htb] 
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{{dddnn}.png}
        \end{minipage}
        \caption{The DDD Network}
        \label{ddd}
    \end{figure}
    In both forward feeding and backward propagation, the gating program will be called to select an appropiate network for the current board extracted features/inputs. Exactly one network will be activate at any time.

    \item \textbf{Meta-Pi Networks -}
    The gating program in the DDD network will suffer from a blemish effect; the stiffness introduced by hard coding the triggers for the networks results in a non-smooth evaluation as noted by Boyan \citeyear{boyan}. The Meta-Pi network is a trainable gating network, figure \ref{metapi}.
    \begin{figure}[htb] 
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{{metapinn}.png}
        \end{minipage}
        \caption{Meta-Pi gating network}
        \label{metapi}
    \end{figure}
    This network will be used to determine the most suited network to be triggered based on a give input. The benefit of this approach is that it will allow the agent to develop a smoother evaluation function. This network will be introduced in later stages of the project once all networks have been trained.
\end{itemize}

\subsection{Training}
A total of 3 networks will be trained; 1 monolithic network and 2 modular networks. The training strategy will be based on the work of Tesauro \citeyear{DBLP:journals/neco/Tesauro94}, Boyan \citeyear{boyan} and Weiring \citeyear{DBLP:journals/jilsa/Wiering10}. The monolithic network and the modular networks with the DDD architecture will be trained by self-play using TD($\lambda$) learning with a decaying learning rate $\alpha$ strating from 0.1 until 0.01 and a decaying value for $\lambda$ starting from 0.9 until 0.7; exponetial decay will be used for both learning rate $\alpha$ and $\lambda$. Each network will be trained on 500,000 games. After each 5000 games, the network will be tested for 1000 games against the previously stored version of the network itself. This will allow to monitor the progress of the network's training. 

Before running the full training with 500,000 games, different configurations for each network will be tested e.g. the addition of expert features as part of the input layer. The number of training games will be reduced to 100,000 and the resultant network will then be tested against the other configurations of that network for 2000 games. The best combination of configurations will be used to fully train each network.

Once DDD networks are finished training, meta-pi gating network will be trained following the same strategy.

\subsection{Evaluation}
Both modular networks will be tested for 5000 games against the monolithic network. The result obtained will give an indication of the general performance of the networks and effictiveness of the strategies. A random sample of those games will be recorded and analysed to evaluate the strategies followed by both modular networks. In addition, both modular networks will be tested for 5000 against each other to meature the performance of the hybrid startegy.

To further evaluate the networks, few test games against an expert-level backgammon human player will be conducted. The expert will be asked to provide feedback about the AI agent's actions and strategy of each modular network. The expert won't be told any details regarding the network that the AI agent will be using at first. After the feedback is obtained from the expert player, they will be made aware of the AI agent that used the network with the new strategy and will be asked to do few more test games and the validate if the network performs a hybrid startegy successfully and it is indeed a new startegy.

\subsection{Extensions}
\subsubsection{Doubling Cube}
As an extension of this project, doubling cube will be included as part of evaulating the actions taken by the AI agent. This will be implemented by including a hueristic function as part of the action evaluation process. The implementation will be initially based on Tesauro's \citeyear{DBLP:journals/ai/Tesauro02} work, then an implementation using Crawford rule will be tested. The best trained network will be retrained with the doubling cube taken into consideration. The newly trained network will be tested for 5000 games against the older version of the network. 

\subsubsection{2-ply search algorithm}
Following Tesauro's \citeyear{DBLP:journals/ai/Tesauro02} work and recent backgammon softwares, the search algorithm used to determine the current turn's move will effect the general training outcome; it is evident from results collected by Depreli \citeyear{botbattle} that better results are expected from 2-ply and 3-ply search. The 2-ply algorithm algorithm will be implemented as follows:

\begin{algorithm}
    \caption{2-ply search}\label{euclid}
    \begin{algorithmic}[1]
    \State $legalActions \gets \textit{getLegalActions()}$
    \State $actions \gets []$
    \For {$action \textit{ in } legalActions$}
    \State $\textit{takeAction}(action)$
    \State $features \gets \textit{extractFeatures}()$
    \State $v \gets \textit{getModelOutput}(features)$ \Comment{get the outcome of the taken action, forward propagation}
    \State $actions.\textit{append}((action,v))$
    \State $\textit{undoAction}(action)$
    \EndFor
    \State $\textit{sortActions}(actions)$ \Comment{sort actions in descending order}
    \State $topActions \gets \textit{getSubList}(actions, 5)$ \Comment{get first five actions}
    \State $actionsOutcome \gets []$
    \For {$action \textit{ in } topActions$}
    \State $\textit{takeAction}(action)$
    \State $outcomes \gets \textit{runAllPossibleDiceRollForOpp}()$ \Comment{20 legal moves considered for each roll, the best outcome of the move of each roll will be returned}
    \State $avgOutcome \gets \textit{avg}(outcomes)$
    \State $actionsOutcome.\textit{append}((action, avgOutcome))$
    \State $\textit{undoAction}(action)$
    \EndFor
    \Return $\textit{getBestRankAction}(actionsOutcome)$ \Comment{best rank action }
    \end{algorithmic}
\end{algorithm}

It is important to note that this computation could take some time and in timed games this could be unfavourable. 

\bibliography{projectpaper}


\end{document}