\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx, subcaption}
\usepackage{algorithm}
\usepackage{url}
\usepackage[noend]{algpseudocode}

\graphicspath{{layouts/} {results/} {gnu-bg/}}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{reinforcement Learning, Looking for New Backgammon Strategies}
\author{Fatema Alkhanaizi}
\student{Fatema Alkhanaizi}
\supervisor{Rob Powell}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\subsection{Context/Background}
Backgammon and many other board games have been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Using reinforcement Learning techniques to play backgammon has given insight to potential strategies that were overlooked in the past.
\subsection{Aims}
The aim of this project is to find a new strategy for backgammon; a hybrid of known strategies will be used as the basis for the new strategy. 
\subsection{Method}
A modular neural network architecture will be used to incorporate the different backgammon strategies. The priming and back games will be used for this project. Two modular networks will be implemented and trained, one that will include the 2 strategies separately and another one that will include a hybrid of the 2 strategies. A single neural network based on TD Gammon will also be implemented and trained. The modular networks will be evaluated against the single network and against each other. Test games against an expert user will be included to validate the new strategy.
\subsection{Results}
A python package that will include modules to train and test the networks using self-play. It will also include a module for setting both a textual and a graphical user interfaces to play against the trained networks. 
\subsection{Conclusions}
\end{abstract}

\begin{keywords}
Backgammon; reinforcement Learning; Modular Neural Network; Priming Game; Back Game; Strategy
\end{keywords}

\section{Introduction}
\subsection{Backgammon}
Backgammon is a game played with dice and checkers on a board consisting of 24 fields, in which each player tries to move their checkers home and bear them off while preventing the opponent from doing the same \cite{glossary}. Figure \ref{bbs} illustrates the setup that was be used for this project.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{boardsetup}.png}
    \end{minipage}
    \caption{Backgammon board setup}
    \label{bbs}
\end{figure}
%% add something about known strategies

There are many know backgammon strategies. The following strategies were the studied in this project:\textbf{The back game}, the player tries to hold two or more anchors, points occupied by two or more checkers in the opponent's home board, as long as possible and force the opponent to bear in or bear off awkwardly \cite{glossary}. \textbf{The priming game}, a particular type of holding game that involves building a prime — a long wall of the player's pieces, ideally 6 points in a row — in order to block the movement of the opponent’s pieces that are behind the wall \cite{glossary}.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime}.png}
    \end{minipage}
    \hspace{1.2em}
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{backgame}.png}
    \end{minipage}
    \caption{Illustration of Priming and Back games}
    \label{games}
\end{figure}

\subsection{Reinforcement Learning}
Backgammon and many other board games have been widely regarded as an ideal testing ground for exploring variety of concepts and approaches in artificial intelligence and machine learning. Reinforcement learning is an often used approach. A reinforcement learning algorithm allows an agent to learn from its experience generated by its interactions with an environment \cite{rl}. 

Using reinforcement Learning techniques to play backgammon had given insight to potential strategies that were overlooked in the past \cite{rl}. TD-Gammon of Tesauro \citeyear{DBLP:conf/icml/Tesauro92} used TD($\lambda$) reinforcement learning methods with a single neural network that trained itself and learned from the outcomes to be an evaluation function for the game of backgammon. (add why this is significant) In backgammon, the dice rolls guaranteed sufficient variability in the games so that all regions of the feature space would be explored. This characteristic made it perfectly suited for learning from self-play as it overcame a known reinforcement Learning problem with the trade-off between exploration and exploitation \cite{survey}. TD($\lambda$) is part of Temporal difference algorithms which is a class of reinforcement learning that uses the difference between two successive positions for backpropagating the evaluations of the successive positions to the current position. TD($\lambda$)-algorithms are parametrised by the value $\lambda$ which makes the agent looks further in the future for updating its value function, strongly influencing the agent learning \cite{rl}.

\subsection{Project Overview}
%% A little light on the background information around backgammon, the different strategies and the use of TD algorithms in different software. 
Extending the work of Tesauro, Boyan \citeyear{boyan} showed that the use of different networks for different subtasks of the game could out perform learning with a single neural network. Many AI software for Backgammon use similar modular neural network architecture i.e. GNU-Backgammon, a top performing open-source software, uses 3 different neural networks for their evaluation function \cite{gnubg}. This project will focus on using modular neural network architecture to incorporate a hybrid of backgammon strategies to find a new game strategy. In addition, the performance of the new strategy will be evaluated by comparing it to another network that includes the strategies separately. Combinations of 2 strategies will be studied: \textbf{the back game} and \textbf{the priming game}.

\subsection{Research Questions}
The following questions were investigated in this project:
\begin{enumerate}
    \item Would using hybrid strategies result in the agent learning a new strategy? or would one strategy be more dominant than the other?
    \item Would including more features for the networks to evaulate result in better strategies?
    \item How effective would the hybrid strategies be over the strategies being included separately?
    \item How would be adding doubling cube to the network evaluation influence the learning outcome?
\end{enumerate}

\subsection{Project Objectives}
Basic objectives: Train a monolithic nueral network based on TD Gammon architecture. Train a modular nueral network that includes primining and back game strategies. Train a modular nueral network that includes hybrid of primining and back game strategies. 
Intermediate objectives: Experiment with the learning parameters and the inclusion of more features extracted from the current state of the game to the networks. Use pubeval as a bench player to evaluate the performance of the networks during training. Use gnubg features to analyse a complete game, to evaluate the moves made by each network; this gives an estimated numerical evaluation to the performance of strategy followed, is it making the best possible moves for the given dice/situation. 
Advanced objectives: Including doubling cube decesion process to the networks. Changing the action selection depth (n-ply algorithm) Increasing the size of the hidden network. 

% It would be nice to learn how the deliverables interact with each other and whether there are some basic, intermediate and advanced ones. 

\subsection{Deliverables}
\begin{itemize}
    \item \textbf{Trained neural networks}: A single neural network based on Tesauro's TD Gammon, a modular neural network that uses a new strategy, a hybrid priming and back strategy, and a modular neural network that uses two separate known strategies, priming and back strategies.
    \item \textbf{AI agent}: This agent will be able to use the trained network to evaluate and make moves.
    \item \textbf{User interface and human agent}: This will be a simple command line interface which takes user inputs to make moves and to play against the trained networks. Another complex implementation will be a graphical user interface which captures the user clicks for making moves.
    \item \textbf{Testing suit}: This will be used to run all tests and evaluations to be done on the networks.
    \item \textbf{Project Report}: All tests and evaluations will be recorded and analysed in this report.
\end{itemize}

\section{Related Work}
This section presents a survey of existing work on the problems that this project addresses.  it should be between 2 to 4 pages in length.  The rest of this section shows the formats of subsections as well as some general formatting information for tables, figures, references and equations.

% include why TD didn't work for other softwares as well as TD Gammon

\section*{Problem Background}
The domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. TD-Gammon  of Tesauro \cite{DBLP:conf/icml/Tesauro92,DBLP:journals/ai/Tesauro02} had demonstrated the impressive ability of machine learning techniques to learn to play games. TD-Gammon used reinforcement learning techniques with a Neural Network (NN) that trains itself to be an evaluation function for the game of backgammon, by playing against itself and learning from the outcome \cite{DBLP:journals/ai/Tesauro02}. eXtreme Gammon \cite{exg}, Snowie \cite{snowie}, and GNUBG \cite{gnubg} are some of the strongest backgammon programs that use Neural Networks; eXtreme Gammon is currently the supreme software \cite{MichaelDepreliStudy2012}. Different variants of backgammon\cite{DBLP:conf/evoW/PapahristouR11,DBLP:conf/ifip12/PapahristouR12}, training techniques, learning methods and neural network architectures have been the focus of some researches that followed the work of Tesauro. This project will focus on studying the effect of including hybrid of Backgammon strategies to the learning network and comparing it to including the strategies separately; the introduction of those strategies will be part of the NN architecture. Many studies do not include the doubling cube in their analyses. As an extension to this project, further analyses with doubling cube will be added.

\section*{Neural Networks Architecture}
The architecture of the NN used for value function evaluation in the reinforcement learning problem plays a big role in the learning speed and performance. A large neural network with more hidden nodes provided a better equity results compared to a smaller network as shown in the work of researchers like Tesauro \cite{DBLP:journals/ai/Tesauro02} and Wiering \cite{DBLP:journals/jilsa/Wiering10}, however the bigger the network the more computations are required thus the slower the learning. GNUBG is made of 3 neural nets: the contact net which is the main net for middlegame positions, the crashed net, and the race net \cite{gnubg}. GNUBG made use of modular neural network \cite{modularNeuralNetwork}, many work that followed TD-gammon made use of this neural network architecture. Backgammon game strategies were incorporated in the NN which had enhanced the results of the NN \cite{DBLP:journals/jilsa/Wiering10,modularNeuralNetwork,gnubg}. 

\section*{Training Techniques}
Training neural networks by self-learning is the most popular training approach that is used in many Backgammon softwares. Tesauro used self-learning to train the NN for TD-gammon; TD-gammon was able to reach master-level \cite{DBLP:conf/icml/Tesauro92}. It has been proven in the work of Wiering \cite{DBLP:journals/jilsa/Wiering10} that by utilising an expert program, the speed of learning increases in comparison to self-learning. However, the end outcome for each self-learning and learning from an expert is almost the same. Learning from watching a match against two experts is by far the worst technique \cite{DBLP:journals/jilsa/Wiering10}.
\subsection*{Doubling Cube}
TD-gammon used a heuristic function for doubling cube (cubeful) computations. The formula that is based on a generalisation of prior theoretical work on doubling strategies by Zadeh?Kobliska \cite{DBLP:journals/ai/Tesauro02}. On the other hand, GNUBG estimated the cubeful equity from the cubeless equity by using a generalised transformations as outlined by Rick Janowski. Both approaches to computing the cubeful equity made us of a formula instead of a neural network as commented by Tesauro and GNUBG creaters the NN approach is more complex so they used the former approach. Snowie \cite{snowie} and eXtreme Gammon \cite{exg} both can evaluate cubeful equities however as both softwares are commercial their approaches are not known.
\section*{Learning Methods}
Other approaches to learning includes genetic programming \cite{DBLP:journals/gpem/AzariaS05}, Co-evolution methods - Hill Climbing \cite{DBLP:journals/ml/PollackB98} and incremental fuzzy clustering method \cite{DBLP:conf/nafips/TabriziKR15}. Pollack and Blair claimed that their Hill Climbing algorithm had 40\% winning factor against Pubeval, a moderately good public-domain player trained by Tesauro using human expert preferences, however, this claim was countered by Tesauro proving that his TD algorithm was better and it deals with nonlinear structures \cite{DBLP:journals/ml/Tesauro98}. This indicated that TD($\lambda$) is the superior learning method. The GP approach have achieved better results against Pubeval of 58\% on the other hand and had shown good results automatically obtained strategies, but it required more computational efforts and was more complex \cite{DBLP:journals/gpem/AzariaS05}. Incremental fuzzy clustering method, an adaptive artificial intelligence method, used in the work of Tabrizi, Khoie and Rahimi was designed to learn to play backgammon from its opponent \cite{DBLP:conf/nafips/TabriziKR15}; learning his movements and tactics which is something none of the previous methods provided, however there is no evidence of this approach being better than the previous learning approaches in terms of playing the game of backgammon. For the focus of this project, TD($\lambda$) algorithm will be used along with modular neural network architecture.

\section{Solution}
\subsection{Algorithms}
\subsubsection{Temporal Difference, TD($\lambda$)}
% this sentense feels like it will be more suited to be part of related work or intro
In backgammon, the dice rolls guarantee sufficient variability in the games so that all regions of the feature space will be explored. This characteristic made it perfectly suited for learning from self-play as it overcomes the known reinforcement Learning problem with the trade-off between exploration and exploitation \cite{survey}. 

TD Gammon used the gradient-decent form of the TD($\lambda$) algorithm with the gradients computed by the error backpropagation algorithm \cite{rl}. The update rule is as follows
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha\delta_t\vec{e}_{t}$$ 
where $\delta_t$ is the TD error,
$$\delta_{t} = r_{t+1} + \gamma V_t(s_{t+1}) - V_t(s_t)$$ 
and $\vec{e}_t$ is a column vector of eligibility traces, one for each component of $\vec{\theta}_t$, updated by 
$$\vec{e}_{t} = \gamma\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)$$ 
The expression $\nabla_{\vec{\theta}_{t}}V_t(s_t)$ refers to the gradient. For backgammon, $\gamma=1$ and the reward is always zero except when winning, reducing the update rule to
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha[V_t(s_{t+1}) - V_t(s_t)][\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)]$$
$\alpha$, the learning rate, and $\lambda$ are constraint by the range $(0,1)$. $e_t(s)$ is the eligibility trace for state $s$, it marks $s$ as eligible for undergoing learning changes when a TD error, $\delta_t$ occurs \cite{rl}.

\subsubsection{Action selection algorithm}
A ply is one turn taken by one user; n-ply refers to how far the player will look ahead when evaluating a move/action \cite{glossary}. The implemented AI agent used a 1-ply search algorithm to pick the best legal action for the current turn; no look ahead. Each action was evaluated in the neural network, forward feeding, and the action with the maximum outcome was returned \cite{DBLP:conf/icml/Tesauro92}. 
% advanges and disadvantages of n-ply or just not include it just yet and add it to the conclusion as part of further work

\subsection{Neural Network Architecture}
\subsubsection{Monolithic Neural Network}
This network was based on Tesauro's \citeyear{DBLP:journals/neco/Tesauro94} TD Gammon implementation; a fully-connected feed-forward neural networks with a single hidden layer. The basic architecture consisted of one input layer I with 288 units for raw inputs representing the checkers configuration on the board,each field in the board is represented by 6 units as there are 24 fields so 144 total units and each player has their own configuration represented separately making the total 288. For basic network architecture the first 4 features from table \ref{exfeat} were used, the other 4 were included at a later stage. Lastly, 2 input units were included to represent if it was the player's turn or the opponent's turn. 
\begin{table}[htb]
    \centering
    \caption{Possible expert features for input layer}
    \vspace*{6pt}
    \label{exfeat}
    \begin{tabular}{cp{12cm}}
        \hline
        \hline
        Feature name & Description \\ 
        \hline
        bar\_pieces\_1 & number of checkers held on the bar for current player\\
        \hline
        bar\_pieces\_2 & number of checkers held on the bar for opponent\\
        \hline
        off\_pieces\_1 & percentage of pieces that current player has borne off \\
        \hline
        off\_pieces\_2 & percentage of pieces that opponent has borne off \\
        \hline
        pip\_count\_1 & pip count for current player \\
        \hline
        pip\_count\_2 & pip count for opponent \\
        \hline
        hit\_pieces\_1 & percentage of pieces that are at risk of being hit (single checker in a position which the opponent can hit) for current player \\
        \hline
        hit\_pieces\_2 & percentage of pieces that are at risk of being hit (single checker in a position which the player can hit) for opponent\\
        \hline
    \end{tabular}
\end{table}
As part of the network architecture, there was one hidden layer H with 50 units and one output layer O with 1 unit representing the winning probability. The number of units for each layer was based on previous implementations, taking into account the complexity introduced by the increase of the units for each layer. The network had weights $w_{ih}$ for all input units $I_i$ to hidden unit $H_h$ and weights $w_{ho}$ for all hidden units $H_h$ to output unit $O_o$. The weights were initialized to random values; hence the initial strategy was a random strategy. Each hidden unit and output unit had a bias $b_h$ and $b_o$. Sigmoid function was used for the activation function between the layers. Each bias was initialized to an array of constant values of $0.1$. Figure \ref{mononn} includes the layout of the neural network.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mononn}.png}
    \end{minipage}
    \caption{Neural Network architecture}
    \label{mononn}
\end{figure}

 A lot of implementations of this network were available in the open source community and were referred throughout the implementation of this project; two code bases from GitHub, backgammon \cite{awni} and td-gammon \cite{fomorians}, were used as the starting point for this project. The main challenge with using open source code was debugging the code and validating it. % mension the problems faced with including these implementations

\subsubsection{Modular Neural Network}
Designer Domain Decomposition Network (DDD) was the Modular Neural Network architectures that was followed in this project. The DDD network consists of n monolithic neural networks and a hard-coded gating program, figure \ref{ddd}. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{dddnn}.png}
    \end{minipage}
    \caption{The DDD Network}
    \label{ddd}
\end{figure}
The gating program, based on the work of Boyan \citeyear{boyan}, partitioned the domain space into n classes. For this project the n classes were represented by the different backgammon strategies. The each strategy was activated when certain board configurations were reached. This approach was implemented by Boyan \citeyear{boyan} and what most recent software such as GNU-Backgammon \cite{gnubg} follow. Both modular networks, seperate strategy and hybrid,i implemented in this project consisted of the following basic networks:
\begin{itemize}
    \item One network for racing game. This network addressed a known weakness of the monolithic network implementation and was only activated when both players' checkers were past each other. 
    \item One default network for all other positions
\end{itemize}
The seperate strategy modular network included the following additional networks:
\begin{itemize}
    \item One network for back game positions: the player is behind in the race, the pip count difference is more than 90 points, but the player has two or more anchors, checkers at one field, in the opponent's home board. This network took into account the number of checkers on the bar.
    \item One network for priming games: if the player has a prime of 4-5 fields, a long wall of checkers, with at least 2 checkers on each field.
\end{itemize}
On the other hand, the hybrid modular network included this network:
\begin{itemize}
    \item One network for a hybrid priming and back game. This network combined the conditions of both back and priming games.
\end{itemize}
In both forward feeding and backward propagation, the gating program was called to select an appropriate network based on the current board configuration and extracted features i.e. pip count. Exactly one network was activated at any time. Each network had the same structure as the monolithic network including the paramaters. Those parameters were modified as necessary as the networks were trained.

\subsection{Implementation}
Python 3 was used as the language for this project. The neural networks was implemented using TensorFlow package. TensorFlow was used as it allowed the generateration of training progress summary, easily save and restore the trained networks through checkpoins. Figure \ref{syscomp} shows the structure and the component dependencies. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{syscomp}.png}
    \end{minipage}
    \caption{System components and dependencies}
    \label{syscomp}
\end{figure}

\subsubsection{Main}
This module was used to invoke other components in the system and handle the general actions required from the system: play, train and test. 

\subsubsection{Game}
This module held the game setup, the rules and constraints of the game e.g. taking an action, finding legal moves and game board setup. An open source implementation taken from a GitHub repository, backgammon \cite{awni}, of this module was refactored and modified for the use in this project. 

\subsubsection{User Interface}
This module handled generating the command line interface (textual interface) and the graphical user interface. The use of either interfaces depended on the availability of pygame, a graphical user interface package for python. Similarly, to the Game module, an open source implementation taken from a GitHub repository, backgammon \cite{awni}, of this module was refactored and modified. 

\subsubsection{Agents}
There are 4 types of agents that were implemented: 
\begin{itemize}
    \item \textbf{Random Agent}, an agent that would randomnly select a legal move. This agent was used in early stages of this project and was replaced by Pubeval agent.
    \item \textbf{Human agent}, an interactive agent which took user inputs either from the command line or by capturing the user clicks though a GUI to make a move/action.
    \item \textbf{AI agent} used a neural network to determine the move/action for the current turn. A list of legal moves is obtained from the game module and the best action was picked based on the search algorithm.
    \item \textbf{Pubeval Agent}, an intermediate level benchmark agent provided by Tesauro \citeyear{pubeval}. It was used to evaluate the performance of the trained networks. Pubeval had two set of weights that it used to select a move: weights for racing game and another for the rest of the positions. 
\end{itemize}

\subsubsection{Modnet}
This module defined the operations for extracting features from the game board and training neural networks. This module heavily depended on Subnet module. An instance of this module was used for the monolithic network. For modular networks, a game-specific gating program was implemented in this module to determine which sub-network was best suited for a given board configuration represented as a set of extracted features. For the different modular neural networks to be trained for this project, different instances of this module were created as each modular network required different gating program.

\subsubsection{Subnet}
This module included the Neural Network implementation using TensorFlow. It provided routines for storing and accessing the network model, checkpoints and summaries generated by TensorFlow. In addition, it included the forward feeding and backpropagation algorithms. All networks created for this project used an instance of this module; networks used in modular neural network and monolithic neural network. An open source implementation taken from a GitHub repository, td-gammon \cite{fomorians}, was used as the basis for this module. 

\subsubsection{Tester}
This module included all evaluations and test routines for the neural networks. 

\subsection{Training}
Each network had 3 types of checkpoints that were used throught the evaluation process of the project: latest, previous, test. The latest checkpoint stored the most recent trained network. The previous checkpoint kept the latest 1,000s game network. The test checkpoint stored all the network configuration after every 1,000s game. For all networks $\lambda$ was set to 0 and $\alpha$ was set to 1 and then to 0.1 after 500 training games.

\subsubsection{Monolithic Nueral Network}
The number of training games was set to 500,000. Based on previous work of Tesauro \citeyear{DBLP:journals/ai/Tesauro02}, and Wiering \citeyear{DBLP:journals/jilsa/Wiering10} the best weights could be reached at a game far before the 500,000 game. After every 1,000 games, the network was tested by playing 100 games against Pubeval agent. This test was set as an indicator of the training progress.

\subsubsection{Modular Nueral Network}
Similarly to the monolithic network training, the number of training games was set to 500,000 and after every 1,000 games, the current network weights would be tested by playing 100 games against Pubeval agent. For modular networks, after every 2,000 games the layout of the game was changed, figure \ref{layouts}. Due to the gating program and the conditions implied for the activation of each sub-network, 4 extra layouts were included: racing layout, prime against prime, back game from the player perspective and back game from the opponent perspective. With these layouts, each sub-network was trained for relatively the same number of games.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{default}.png}
        \subcaption{Basic layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{o_backgame_layout}.png}
        \subcaption{Player back game layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{x_backgame_layout}.png}
        \subcaption{Opponent back game layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime_layout}.png}
        \subcaption{Prime against prime layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{racing_game_layout}.png}
        \subcaption{Racing layout}
    \end{minipage}
    \caption{Modular Network training layouts}
    \label{layouts}
\end{figure}

\section{Results}
All Networks were trained on machines with the following specifications: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz with 32.0GB RAM and Hyber-V support. Multiple learning parameters and input features were tested which required the networks to be retrained with each change. To measure the performance of the networks pubeval agent was used. To analyse the strategies of the networks, GNU-Backgammon software was used. The analysis by the software provided details regarding the points gained or lost as a result of a move made in the current turn. There were many agents with varied strengths provided by the software. The world class agent was picked to provide a good challenge for the trained networks; it plays a really strong game close to the best human players in the world by using 2-ply lookahead, no noise, and a normal move filter \cite{gnubg}.
\subsection{General performance}
First, variace parameters for $\lambda$ and $\alpha$ were tested. The values used were based on previous related works. The basic input features were used for all cases. Initially, the random agent was used as the benchmark player, however after experimenting with different paramaters, the random player proved to be too weak so it was disregarded as a testing agent. Pubeval agent was used for the remainder of the tests, table \ref{rp}.
\begin{table}[htb]
    \centering
    \caption{Monolithic network performance after 1000 test games against pubeval (P) and random (R) agents}
    \label{rp}
    \begin{tabular}{ccc|ccc}
        \hline
        \hline
        \multicolumn{3}{c|}{$\lambda=0.7, \alpha=0.01$} & \multicolumn{3}{|c}{$\lambda=0, \alpha=0.1$} \\
        Max after & Win rate (P) & Win rate (R) & Max after & Win rate (R) & Win rate (P)  \\ 
        \hline
        100,000 & 66.0\% & 3.0\% & 159,000 & 98.0\% & 60.00\% \\ 
        \hline
    \end{tabular}
\end{table}
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-0.7-random}.png}
        \subcaption{Basic features}
        \label{mono:7}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{pip2}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \caption{every 1000s trained game against Random agent for 100 test games}
    \label{plot}
\end{figure}
When $\lambda \leq 0.7$ the outcome did not change significantly and only resulted in a faster convergance for the network; in other words the training stabilized as shown in figure \ref{mono:7} with the agent winning 66\% against the random, however the performance was rather bad against pubeval. When $\lambda$ was 0, performance significantly improved after the first 1000 games and imp

\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-non}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-pip}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{every 1000s trained game for monolithich network against pubeval agent for 100 test games}
    \label{plot}
\end{figure}

\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-non}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-pip}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{every 1000s trained game for seperate modular network against pubeval agent for 100 test games}
    \label{plot}
\end{figure}

\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-non}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-pip}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{every 1000s trained game for hybrid modular network against pubeval agent for 100 test games}
    \label{plot}
\end{figure}

Table \ref{perf1} shows the performance of the networks with basic input features, with pip count included to the basic features and with both pip count and hit probability included to the basic features against Pubeval agent after 1000 test games. $\lambda$ and $\alpha$ were not changed. The networks were left to run for as long as possible. The result in the tables were for the latest training game reached by each network. 
% add graphs for the test outputs
\begin{table}[htb]
    \centering
    \caption{Performance after 1000 test games against Pubeval agent}
    \label{perf1}
    \begin{tabular}{c|cc|cc|cc}
        \hline
        \hline
        & \multicolumn{2}{|c|}{Basic} & \multicolumn{2}{|c|}{Pip count} & \multicolumn{2}{|c}{Pip count + hit probability} \\
        Architecture & Max after & Win rate & Max after & Win rate & Max after & Win rate \\ 
        \hline
        Monolithic & 159,000 & 60.00\% & 94,000 & 54.00\% & 100,000 & 60.00\% \\ 
        \hline
        Seperate Modular & 111,000 & 38.20\% & 34,000 & 16.20\%  & 12,000 & 24.20\% \\ 
        \hline
        Hybrid Modular & 46,000 & 19.30\% & 143,000 & 35.30\% & 29,000 & 20.00\% \\ 
        \hline
    \end{tabular}
\end{table}

\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{non2}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{pip2}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{all2}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{every 1000s trained game against pubeval agent for 100 test games}
    \label{plot}
\end{figure}

As noted from figure , the training did not improve as the network's training continued and tended to get worst in some cases. 


\subsection{Match analysis}
The best performing networks were tested against GNU-Backgammon's world class level agent for 10 games. 
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-racing-board}.png}
        \subcaption{Game Board}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-racing-rollout}.png}
        \subcaption{Move analysis}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-feedback}.png}
        \subcaption{Full game analysis}
        \label{plot:all}
    \end{minipage}
    \caption{Monolithic network (Red) against GNU-Backgammon world class level agent (Black) - racing game}
    \label{plot}
\end{figure}
For monolithic network, as the game reaches to the racing game, the network makes average decisions based on the analysis, picking moves that are based on GNU-Backgammon in the middle range 17th move picked out of 25 possible moves. In some cases it makes aweful decisions but generally it picks one of 3 top moves. This empahsises the results obtained previously regarding the weakness of a single network implementation. The monolithic network tended to use primes as part of its strategy. This is noticed by watchign the network play against Pubeval 

For hybrid network, plays good first roll positions, always makes the best decision when racing game is reached. it makes awful decisions but generally it picks one of 3 top moves.  



\section{Evaluation}
\subsection{Strength and limitations}
The results for the monolithic network indicates that it is better than the original td gammon. however the modular networks did not give the expected results and generally performed worst than the monolithic network.

Time has been the major limitation for this project. For each network. For fast games with monolithic network around 65,000 games can be trained per day. This puts major limitation when testing different parameters and any errors in the code waste days of training. Hardware limitations are one major aspect and highly influences training time. Training required a fast CPU to reach the goal of 500,000 games. 

Bug are difficult to spot. As training takes time and good result happen after a couple of days, any bug can't be spotted until after days of training had gone by. 

Modular Networks. Some games could take thousands of turns, this usually indicates that the network has diverged this incredebly increase the time a single game takes slowing down the training significantly. Although adjusting paramaters helps in some cases, convergences is not always garanteed and different runs for the same configurations could go either ways. This also explains the low number of games that were trained for those networks. Inclusion of expert however seemed to have fixed the divergance problem giving better results. 

Action selection algorithm could be to blame for this behaviour as the first best action was always selected. 

The results indicate that the modular networks did not out perform the monolithic network. however the hybrid module network did better than the seperate modula network. 

It is difficult to decided when is it appropiate to stop learning. 

Difficulty in interacting between the agent and gnu agent. required more manual effort which was incredibly ineffective and slow. Manually entering dice roll and entering move provided by both players.

\subsection{Improvements}
If given a chance to repeat this project the advance objectives would have been included. Training strategy for the network re-evaluated. As most bugs in the code has been fixed and a better evaluation method had been included with using pubeval. Doubling cube will be included as part of evaluating the actions taken by the AI agent. This will be implemented by including a heuristic function as part of the action evaluation process. The implementation will be initially based on Tesauro's \citeyear{DBLP:journals/ai/Tesauro02} work, then an implementation using Crawford rule will be tested. The best trained network will be retrained with the doubling cube taken into consideration. The newly trained network will be tested for 5000 games against the older version of the network. 
Following Tesauro's \citeyear{DBLP:journals/ai/Tesauro02} work and recent backgammon software, the search algorithm used to determine the current turn's move will affect the general training outcome; it is evident from results collected by Depreli \citeyear{botbattle} that better results are expected from 2-ply and 3-ply search. 

Use different package that is more optimized to use the CPU improving the training time.

\section{Conclusions}

\bibliography{projectpaper}


\end{document}