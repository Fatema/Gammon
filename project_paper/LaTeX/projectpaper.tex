\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx, subcaption}
\usepackage{algorithm}
\usepackage{url}
\usepackage[noend]{algpseudocode}

\graphicspath{{layouts/} {results/} {gnu-bg/}}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{reinforcement Learning, Looking for New Backgammon Strategies}
\author{Fatema Alkhanaizi}
\student{Fatema Alkhanaizi}
\supervisor{Rob Powell}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\subsection{Context/Background}
Using reinforcement learning tenchniques had allowed backgammon players to consider new game strategies. A nueral network is used to evaluate the actions. Different architectures result in different strategies. 
\subsection{Aims}
The aim of this project is to find a new strategy for backgammon; a hybrid of known strategies will be used as the basis for the new strategy. 
\subsection{Method}
A modular neural network architecture will be used to incorporate the different backgammon strategies. The priming and back games will be used for this project. Two modular networks will be implemented and trained, one that will include the 2 strategies separately and another one that will include a hybrid of the 2 strategies. A single neural network based on TD Gammon will also be implemented and trained. The modular networks will be evaluated against the single network and against each other. Test games against an expert user will be included to validate the new strategy.
\subsection{Results}
A python package that will include modules to train and test the networks using self-play. It will also include a module for setting both a textual and a graphical user interfaces to play against the trained networks. 
\subsection{Conclusions}
Although the solution was flawed there is much room for improvments.
\end{abstract}

\begin{keywords}
Backgammon; reinforcement Learning; Modular Neural Network; Priming Game; Back Game; Strategy
\end{keywords}

\section{Introduction}
\subsection{Backgammon}
Backgammon is a game played with dice and checkers on a board consisting of 24 fields, in which each player tries to move their checkers home and bear them off while preventing the opponent from doing the same \cite{glossary}. Figure \ref{bbs} illustrates the setup that was be used for this project.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{boardsetup}.png}
    \end{minipage}
    \caption{Backgammon board setup}
    \label{bbs}
\end{figure}
%% A little light on the background information around backgammon, the different strategies 
%% add something about known strategies

There are many know backgammon strategies. The following strategies were the studied in this project:\textbf{The back game}, the player tries to hold two or more anchors, points occupied by two or more checkers in the opponent's home board, as long as possible and force the opponent to bear in or bear off awkwardly \cite{glossary}. \textbf{The priming game}, a particular type of holding game that involves building a prime — a long wall of the player's pieces, ideally 6 points in a row — in order to block the movement of the opponent’s pieces that are behind the wall \cite{glossary}.
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime}.png}
    \end{minipage}
    \hspace{1.2em}
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{backgame}.png}
    \end{minipage}
    \caption{Illustration of Priming and Back games}
    \label{games}
\end{figure}

\subsection{Reinforcement Learning}
Backgammon and many other board games have been widely regarded as an ideal testing ground for exploring variety of concepts and approaches in artificial intelligence and machine learning. Reinforcement learning is an often used approach. A reinforcement learning algorithm allows an agent to learn from its experience generated by its interactions with an environment. Temporal difference is a class of reinforcement learning that uses the difference between two successive positions for backpropagating the evaluations of the successive positions to the current position. TD($\lambda$) algorithms are part of Temporal difference algorithms that are parametrised by the value $\lambda$ which makes the agent looks further in the future for updating its value function, strongly influencing the agent learning \cite{rl}. TD-Gammon of Tesauro \citeyear{DBLP:conf/icml/Tesauro92} used TD($\lambda$) reinforcement learning methods with a single neural network that trained itself and learned from the outcomes to be an evaluation function for the game of backgammon. Due to the branching factor for backgammon, about 400, the conventional huesristic search methods that were effective for games such as chess and checkers couldn't be applied to backgammon \cite{rl}. Making backgammon more suited for Temporal difference learning methods. In backgammon, the dice rolls guaranteed sufficient variability in the games so that all regions of the feature space would be explored. This characteristic made it also perfectly suited for learning from self-play as it overcame a known reinforcement Learning problem with the trade-off between exploration and exploitation \cite{survey}. TD-Gammon's network required little backgammon knowledge, but still achived a near level of the world's strongest grandmaster. This outcome had shown that using reinforcment learning methods to play stochastic games like backgammon was proming. In addition, TD-Gammon's strength had given insight to potential backgammon strategies that were not considered by experts during that time. 

\subsection{Project Overview}
TD-Gammon lacked in some aspect of the game like the racing game, this was improved by using modular nueral network architecture for backgammon highlighted by the work of Boyan \citeyear{boyan}. This project will focus on using modular neural network architecture to incorporate a hybrid of backgammon strategies to find a new game strategy. In addition, the performance of the new strategy will be evaluated by comparing it to another network that includes the strategies separately. Combinations of 2 strategies will be studied: \textbf{the back game} and \textbf{the priming game}.

\subsection{Research Questions}
The following questions were investigated in this project:
\begin{enumerate}
    \item Would using hybrid strategies result in the agent learning a new strategy? or would one strategy be more dominant than the other?
    \item Would including more features for the networks to evaulate result in better strategies?
    \item How effective would the hybrid strategies be over the strategies being included separately?
    \item How would be adding doubling cube to the network evaluation influence the learning outcome?
\end{enumerate}

\subsection{Project Objectives}
Basic objectives: Train a monolithic nueral network based on TD Gammon architecture. Train a modular nueral network that includes primining and back game strategies. Train a modular nueral network that includes hybrid of primining and back game strategies. 
Intermediate objectives: Experiment with the learning parameters and the inclusion of more features extracted from the current state of the game to the networks. Use pubeval as a bench player to evaluate the performance of the networks during training. Use gnubg features to analyse a complete game, to evaluate the moves made by each network; this gives an estimated numerical evaluation to the performance of strategy followed, is it making the best possible moves for the given dice/situation. 
Advanced objectives: Including doubling cube decesion process to the networks. Changing the action selection depth (n-ply algorithm) Increasing the size of the hidden network. 
All basic and intermediate objectives were reached. The advanced objectives were left for future work. 
% include what was achieved

% It would be nice to learn how the deliverables interact with each other and whether there are some basic, intermediate and advanced ones. 

\subsection{Deliverables}
\begin{itemize}
    \item \textbf{Trained neural networks}: A single neural network based on Tesauro's TD Gammon, a modular neural network that uses a new strategy, a hybrid priming and back strategy, and a modular neural network that uses two separate known strategies, priming and back strategies.
    \item \textbf{AI agent}: This agent was able to use the trained network to evaluate and make moves.
    \item \textbf{User interface and human agent}: This was a simple command line interface which took user inputs to make moves and to play against the trained networks. Another complex implementation was a graphical user interface which captured the user clicks for making moves.
    \item \textbf{Testing suit}: This was used to run all tests and evaluations to be done on the networks.
    \item \textbf{Project Report}: All tests and evaluations were recorded and analysed in this report.
\end{itemize}

\section{Related Work}
% include why TD didn't work for other softwares as well as TD Gammon, the use of TD algorithms in different software.
Many research had been conducted on how to develop a strong backgammon AI agent. Early research had used hueristic approches and supervised learning methods, however temporal difference learning methods proved to be superior and resulted in stronger agents surpasing the world's strongest Backgammon players \cite{DBLP:journals/ai/Tesauro02}. Approaches that used temporal difference methods, also used nueral networks for the  value evaluation function. The architecture of the networks influenced the strength observed for the agent. Complexity and depth of action selection algorithm also played a part in the overall performance. In this section, overview of the current state of art implemenetations, the nueral network architectures used, training techniques and other learning approaches are discussed. 
\subsection{State of the art implemenetations}
TD-Gammon of Tesauro \citeyear{DBLP:conf/icml/Tesauro92,DBLP:journals/ai/Tesauro02} had demonstrated the impressive ability of machine learning techniques to learn to play games.  TD-Gammon used reinforcement learning techniques with a Neural Network that trained itself to be an evaluation function for the game of backgammon, by playing against itself and learning from the outcome \cite{DBLP:journals/ai/Tesauro02}. This made TD-Gammon a point of interest to many following research. Many had tried to replicate the success of TD-Gammon by applying Temporal difference learning to other game such as chess, otello and Go, but non reached the same level of succuss as TD-Gammon \cite{survey}. The stochastic nature of backgammon made it suited for temporal difference learning. Extending the work of Tesauro, Boyan \citeyear{boyan} showed that the use of different networks for different subtasks of the game could out perform learning with a single neural network. Many AI software for Backgammon use similar modular neural network architecture such as GNU-Backgammon, a top performing open-source software, uses 3 different neural networks for their evaluation function \cite{gnubg}. Modular network architecture seems to be the way forward.

\subsection{Neural Networks Architecture}
The architecture of the nueral network used for the backgammon agent plays a big role in the netwok's strength and performance. A large neural network with more hidden nodes provided better results compared to a smaller network as shown in the work of researchers like Tesauro \citeyear{DBLP:journals/ai/Tesauro02} and Wiering \citeyear{DBLP:journals/jilsa/Wiering10}, however the bigger the network the more computations are required thus the slower the learning. The learning parameters used also influences the learning. Results from the work of Tesauro \citeyear{DBLP:journals/ai/Tesauro02} indicated that with $\lambda$ being between 0.6 and 0.7 best performance was observed; Tesauro \citeyear{pubeval} reported a 0.596 performance against a benchmark agent pubeval which he made available for the community to use. Tesauro developed pubeval himself and had remarked that it plays at intermediate player level. In another research looking at different variance of backgammon, $\lambda=0$ gave much better performance \cite{DBLP:conf/ifip12/PapahristouR12}. TD-Gammon was rather strong agent, but it ignored important aspects of backgammon like the running game \cite{survey}. Boyan's \citeyear{boyan} modular nueral network architecture had proven to be superior than the monolithic neural network architecture used in TD Gammon, overcoming its observed weakness. He used 2 types of modular architectures: Designer Domain Decomposition (DDD) architecture and Meta-Pi architecture. His DDD architecture made use of a gating program that partitioned the space of backgammon positions. Each partition had its own nueral network. He noted that better partitioning of backgammon positions in the gating program could improve the performance. This architecture seems reasonable to be used to include different backgammon strategies, making the strategies act as partitions for the positions space. The Meta-Pi architecture delt with replacing the gating program with a nueral network to deal with activating the partition networks. It did not however improve performance on his DDD network. As for the best known agent, GNU-Backgammon is made of 3 neural nets: the contact net which is the main net for middlegame positions, the crashed net, and the race net \cite{gnubg}. This further proves the suitibility of modular architecture for including backgammon strategies to the learning agent.

\subsection{Training Techniques}
Training neural networks by self-learning is the most popular training approach that is used in many Backgammon softwares. Tesauro used self-learning to train the NN for TD-gammon; TD-gammon was able to reach master-level \cite{DBLP:conf/icml/Tesauro92}. It has been proven in the work of Wiering \cite{DBLP:journals/jilsa/Wiering10} that by utilising an expert program, the speed of learning increases in comparison to self-learning. However, the end outcome for each self-learning and learning from an expert is almost the same. Learning from watching a match against two experts is by far the worst technique \cite{DBLP:journals/jilsa/Wiering10}.
TD-gammon used a heuristic function for doubling cube (cubeful) computations. The formula that is based on a generalisation of prior theoretical work on doubling strategies by Zadeh?Kobliska \cite{DBLP:journals/ai/Tesauro02}. On the other hand, GNUBG estimated the cubeful equity from the cubeless equity by using a generalised transformations as outlined by Rick Janowski. Both approaches to computing the cubeful equity made us of a formula instead of a neural network as commented by Tesauro and GNUBG creaters the NN approach is more complex so they used the former approach. Snowie \cite{snowie} and eXtreme Gammon \cite{exg} both can evaluate cubeful equities however as both softwares are commercial their approaches are not known.

\subsection{Other learning approaches}
Other approaches to learning includes genetic programming \cite{DBLP:journals/gpem/AzariaS05}, Co-evolution methods - Hill Climbing \cite{DBLP:journals/ml/PollackB98} and incremental fuzzy clustering method \cite{DBLP:conf/nafips/TabriziKR15}. Pollack and Blair claimed that their Hill Climbing algorithm had 40\% winning factor against Pubeval, a moderately good public-domain player trained by Tesauro using human expert preferences, however, this claim was countered by Tesauro proving that his TD algorithm was better and it deals with nonlinear structures \cite{DBLP:journals/ml/Tesauro98}. This indicated that TD($\lambda$) is the superior learning method. The GP approach have achieved better results against Pubeval of 58\% on the other hand and had shown good results automatically obtained strategies, but it required more computational efforts and was more complex \cite{DBLP:journals/gpem/AzariaS05}. Incremental fuzzy clustering method, an adaptive artificial intelligence method, used in the work of Tabrizi, Khoie and Rahimi was designed to learn to play backgammon from its opponent \cite{DBLP:conf/nafips/TabriziKR15}; learning his movements and tactics which is something none of the previous methods provided, however there is no evidence of this approach being better than the previous learning approaches in terms of playing the game of backgammon. For the focus of this project, TD($\lambda$) algorithm will be used along with modular neural network architecture.

\section{Solution}
\subsection{Algorithms}
\subsubsection{Temporal Difference, TD($\lambda$)}
% this sentense feels like it will be more suited to be part of related work or intro
In backgammon, the dice rolls guarantee sufficient variability in the games so that all regions of the feature space will be explored. This characteristic made it perfectly suited for learning from self-play as it overcomes the known reinforcement Learning problem with the trade-off between exploration and exploitation \cite{survey}. 

TD Gammon used the gradient-decent form of the TD($\lambda$) algorithm with the gradients computed by the error backpropagation algorithm \cite{rl}. The update rule is as follows
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha\delta_t\vec{e}_{t}$$ 
where $\delta_t$ is the TD error,
$$\delta_{t} = r_{t+1} + \gamma V_t(s_{t+1}) - V_t(s_t)$$ 
and $\vec{e}_t$ is a column vector of eligibility traces, one for each component of $\vec{\theta}_t$, updated by 
$$\vec{e}_{t} = \gamma\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)$$ 
The expression $\nabla_{\vec{\theta}_{t}}V_t(s_t)$ refers to the gradient. For backgammon, $\gamma=1$ and the reward is always zero except when winning, reducing the update rule to
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} + \alpha[V_t(s_{t+1}) - V_t(s_t)][\lambda\vec{e}_{t-1} + \nabla_{\vec{\theta}_{t}}V_t(s_t)]$$
$\alpha$, the learning rate, and $\lambda$ are constraint by the range $(0,1)$. $e_t(s)$ is the eligibility trace for state $s$, it marks $s$ as eligible for undergoing learning changes when a TD error, $\delta_t$ occurs \cite{rl}. This exact algorithm was used in this project.

\subsubsection{Action selection algorithm}
A ply is one turn taken by one user; n-ply refers to how far the player will look ahead when evaluating a move/action \cite{glossary}. The implemented AI agent used a 1-ply search algorithm, \cite{DBLP:conf/icml/Tesauro92}, to pick the best legal action for the current turn; no look ahead. Each action was evaluated in the neural network, forward feeding, and the action with the maximum outcome was returned. 
% advanges and disadvantages of n-ply or just not include it just yet and add it to the conclusion as part of further work

\subsection{Neural Network Architecture}
\subsubsection{Monolithic Neural Network}
This network was based on Tesauro's \citeyear{DBLP:journals/neco/Tesauro94} TD Gammon implementation; a fully-connected feed-forward neural networks with a single hidden layer. The basic architecture consisted of one input layer I with 288 units for raw inputs representing the checkers configuration on the board,each field in the board is represented by 6 units as there are 24 fields so 144 total units and each player has their own configuration represented separately making the total 288. For basic network architecture the first 4 features from table \ref{exfeat} were used, the other 4 were included at a later stage. Lastly, 2 input units were included to represent if it was the player's turn or the opponent's turn. 
\begin{table}[htb]
    \centering
    \caption{Possible expert features for input layer}
    \vspace*{6pt}
    \label{exfeat}
    \begin{tabular}{cp{12cm}}
        \hline
        \hline
        Feature name & Description \\ 
        \hline
        bar\_pieces\_1 & number of checkers held on the bar for current player\\
        \hline
        bar\_pieces\_2 & number of checkers held on the bar for opponent\\
        \hline
        off\_pieces\_1 & percentage of pieces that current player has borne off \\
        \hline
        off\_pieces\_2 & percentage of pieces that opponent has borne off \\
        \hline
        pip\_count\_1 & pip count for current player \\
        \hline
        pip\_count\_2 & pip count for opponent \\
        \hline
        hit\_pieces\_1 & percentage of pieces that are at risk of being hit (single checker in a position which the opponent can hit) for current player \\
        \hline
        hit\_pieces\_2 & percentage of pieces that are at risk of being hit (single checker in a position which the player can hit) for opponent\\
        \hline
    \end{tabular}
\end{table}
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mononn}.png}
    \end{minipage}
    \caption{Neural Network architecture}
    \label{mononn}
\end{figure}

As part of the network architecture, there was one hidden layer H with 50 units and one output layer O with 1 unit representing the winning probability. The number of units for each layer was based on previous implementations, taking into account the complexity introduced by the increase of the units for each layer. The network had weights $w_{ih}$ for all input units $I_i$ to hidden unit $H_h$ and weights $w_{ho}$ for all hidden units $H_h$ to output unit $O_o$. The weights were initialized to random values; hence the initial strategy was a random strategy. Each hidden unit and output unit had a bias $b_h$ and $b_o$. Sigmoid function was used for the activation function between the layers. Each bias was initialized to an array of constant values of $0.1$. Figure \ref{mononn} includes the layout of the neural network.

 A lot of implementations of this network were available in the open source community and were referred throughout the implementation of this project; two code bases from GitHub, backgammon \cite{awni} and td-gammon \cite{fomorians}, were used as the starting point for this project. The main challenge with using open source code was debugging the code and validating it. % mension the problems faced with including these implementations

\subsubsection{Modular Neural Network}
Designer Domain Decomposition Network (DDD) was the Modular Neural Network architectures that was followed in this project. The DDD network consists of n monolithic neural networks and a hard-coded gating program, figure \ref{ddd}. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{dddnn}.png}
    \end{minipage}
    \caption{The DDD Network}
    \label{ddd}
\end{figure}
The gating program, based on the work of Boyan \citeyear{boyan}, partitioned the domain space into n classes. For this project the n classes were represented by the different backgammon strategies. The each strategy was activated when certain board configurations were reached. This approach was implemented by Boyan \citeyear{boyan} and what most recent software such as GNU-Backgammon \cite{gnubg} follow. Both modular networks, seperate strategy and hybrid, implemented in this project consisted of the following basic networks:
\begin{itemize}
    \item One network for racing game. This network addressed a known weakness of the monolithic network implementation and was only activated when both players' checkers were past each other. 
    \item One default network for all other positions
\end{itemize}
The seperate strategy modular network included the following additional networks:
\begin{itemize}
    \item One network for back game positions: the player is behind in the race, the pip count difference is more than 90 points, but the player has two or more anchors, checkers at one field, in the opponent's home board. This network took into account the number of checkers on the bar.
    \item One network for priming games: if the player has a prime of 4-5 fields, a long wall of checkers, with at least 2 checkers on each field.
\end{itemize}
On the other hand, the hybrid modular network included this network:
\begin{itemize}
    \item One network for a hybrid priming and back game. This network combined the conditions of both back and priming games.
\end{itemize}
In both forward feeding and backward propagation, the gating program was called to select an appropriate network based on the current board configuration and extracted features i.e. pip count. Exactly one network was activated at any time. Each network had the same structure as the monolithic network including the paramaters.

% talk about the disadvantages of using the hard-coded gating program and why it was used. For ease of implemenetation and testing variace parameters and features, the hard coded gating program was used. Ideally this program would be replaced by another nueral network to handle the activation of the sub-networks; this is Meta-Pi architecture \cite{boyan}. 

\subsection{Implementation}
Python 3 was used as the language for this project. The neural networks was implemented using TensorFlow package. TensorFlow was used as it allowed the generateration of training progress summary, easily save and restore the trained networks through checkpoins. Figure \ref{syscomp} shows the structure and the component dependencies. 
\begin{figure}[htb] 
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{syscomp}.png}
    \end{minipage}
    \caption{System components and dependencies}
    \label{syscomp}
\end{figure}

\subsubsection{Main}
This module was used to invoke other components in the system and handle the general actions required from the system: play, train and test. 

\subsubsection{Game}
This module held the game setup, the rules and constraints of the game e.g. taking an action, finding legal moves and game board setup. An open source implementation taken from a GitHub repository, backgammon \cite{awni}, of this module was refactored and modified for the use in this project. 

\subsubsection{User Interface}
This module handled generating the command line interface (textual interface) and the graphical user interface. The use of either interfaces depended on the availability of pygame, a graphical user interface package for python. Similarly, to the Game module, an open source implementation taken from a GitHub repository, backgammon \cite{awni}, of this module was refactored and modified. 

\subsubsection{Agents}
There are 4 types of agents that were implemented: 
\begin{itemize}
    \item \textbf{Random Agent}, an agent that would randomnly select a legal move.
    \item \textbf{Human agent}, an interactive agent which took user inputs either from the command line or by capturing the user clicks though a GUI to make a move/action.
    \item \textbf{AI agent} that used a neural network to determine the move/action for the current turn. A list of legal moves was obtained from the game module and the best action was selected based on the search algorithm.
    \item \textbf{Pubeval Agent}, an intermediate level benchmark agent provided by Tesauro \citeyear{pubeval}. Pubeval had two set of weights that it used to select a move: weights for racing game and weights for the remaining positions. 
\end{itemize}

\subsubsection{Modnet}
This module defined the operations for extracting features from the game board and training neural networks. This module heavily depended on Subnet module. An instance of this module was used for the monolithic network. For modular networks, a game-specific gating program was implemented in this module to determine which sub-network was best suited for a given board configuration represented as a set of extracted features. For the different modular neural networks to be trained for this project, different instances of this module were created as each modular network required different gating program.

\subsubsection{Subnet}
This module included the Neural Network implementation using TensorFlow. It provided routines for storing and accessing the network model, checkpoints and summaries generated by TensorFlow. In addition, it included the forward feeding and backpropagation algorithms. All networks created for this project used an instance of this module; networks used in modular neural network and monolithic neural network. An open source implementation taken from a GitHub repository, td-gammon \cite{fomorians}, was used as the basis for this module. 

\subsubsection{Tester}
This module included all evaluations and test routines for the neural networks. 

\subsection{Strategy Validation}
The main aim of this project was to find a new backgammon strategy. To validate the strategy followed by the networks, a measure for the general performance of the strategy against a benchmark agent was taken, and few complete matches for the best networks were analysised. Before making any measurements or analysis, it was important to determine at which stage should the training stop i.e. convergence of the network. There were 3 indicators that determined the convergance of a network: consistent test games results, total number of turns taken and convergance of the loss plot, figure \ref{conv}. The loss plot was for the mean squared of TD error, $\delta_t$. A low total for number of turns per games indicated that the training was going in the right direction \cite{DBLP:journals/ai/Tesauro02}; very long games, over 500 turns, were a red flag.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-all}.png}
        \subcaption{Consistent test games results}
        \label{mono:1}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-turns}.png}
        \subcaption{Low number of turns for each game}
        \label{mono:2}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-loss-plot}.png}
        \subcaption{Converged loss plot}
        \label{mono:3}
    \end{minipage}
    \caption{Convergance indicators}
    \label{conv}
\end{figure}

To measure the performance of the networks, the random agent was used in early stages of this project but it was replaced by pubeval agent as it was too weak. To analyse the strategies of the networks, GNU-Backgammon software was used. GNU-Backgammon software provided a complete analysis highlighting bad moves in red and marking them with question marks \ref{gnubg}. Further analysis was provided by clicking on the move with a breakdown of the points gained or lost by making that move and its ranking. There were many agents with varied strengths provided by the software. The world class agent was picked to provide a good challenge for the trained networks; it played a really strong game close to the best human players in the world by using 2-ply lookahead, no noise, and a normal move filter \cite{gnubg}. 
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.88\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{gnubg}.png}
        \label{mono:1}
    \end{minipage}
    \caption{GNU Backgammon software}
    \label{gnubg}
\end{figure}

\subsection{Training and Testing}
Each network had 3 types of checkpoints that were used throught the evaluation process of the project: latest, previous, test. The latest checkpoint stored the most recent trained network. The previous checkpoint stored the latest 1,000s game trained network. The test checkpoint stored all the trained networks after every 1,000s game. 

\subsubsection{Monolithic Nueral Network}
The number of training games was set to 500,000. Based on previous work of Tesauro \citeyear{DBLP:journals/ai/Tesauro02}, and Wiering \citeyear{DBLP:journals/jilsa/Wiering10}, the best weights could be reached at a game far before the 500,000 game. After every 1,000 games, the network was tested by playing 100 games against Pubeval agent. This test was set as an indicator of the training progress.

\subsubsection{Modular Nueral Network}
Similarly to the monolithic network training, the number of training games was set to 500,000 and after every 1,000 games, the current network would be tested by playing 100 games against Pubeval agent. However, for modular networks, after every 2,000 games the layout of the game was changed, figure \ref{layouts}. Due to the gating program and the conditions implied for the activation of each sub-network, 4 extra layouts were included: racing layout, prime against prime, back game from the player perspective and back game from the opponent perspective. With these layouts, each sub-network was trained for relatively the same number of games. 
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{default}.png}
        \subcaption{Basic layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{o_backgame_layout}.png}
        \subcaption{Player back game layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{x_backgame_layout}.png}
        \subcaption{Opponent back game layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{prime_layout}.png}
        \subcaption{Prime against prime layout}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{racing_game_layout}.png}
        \subcaption{Racing layout}
    \end{minipage}
    \caption{Modular Network training layouts}
    \label{layouts}
\end{figure}

\section{Results}
In this section, the results of all networks' training are presented. All Networks were trained on machines with the specifications outlined in table \ref{spec}. For every parameter and feature experimented, the networks were retrained  for relatively the same time duration to reach the goal of 500,000 trained games. Monolithic networks were estimated to take 1 week while modular networks were estimated to take 2 weeks; these estimates were made by considering the number of games ran in an hour, it was around 3,100 games per hour for monolithic network and around 2,000 games per hour for modular networks. Each network used approximetely 1 core of the machine, this allowed for 4 instances of the networks to run at the same time on a single machine. Instances were run multiple times in case of divergence or test results were lower than 10\% and no progress was observed within a weeks time.
\begin{table}[htb]
    \centering
    \caption{environment system specification}
    \label{spec}
    \begin{tabular}{cc}
        \hline
        \hline
        Component & Description \\ 
        \hline
        Processor & Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz \\ 
        \hline
        Ram & 32.00GB \\ 
        \hline
        Operating system & Windows 10 \\ 
        \hline
    \end{tabular}
\end{table}

\subsection{General performance}
In the first stage of experiments, parameters for $\lambda$ and $\alpha$ were varied. The values used were based on previous related works. The basic input features were used in all experiments. Initially, the random agent was used as the benchmark player. However after experimenting with different paramaters, the random agent proved to be too weak, so it was disregarded as a testing agent, table \ref{rp}. Pubeval agent was used for the remainder of the tests.
\begin{table}[htb]
    \centering
    \caption{Best trained parameters after 1000 test games against pubeval (P) and random (R) agents}
    \label{rp}
    \begin{tabular}{c|ccc|ccc}
        & \multicolumn{3}{c}{$\lambda=0.7, \alpha=0.01$} & \multicolumn{3}{|c}{$\lambda=0, \alpha=1, 0.1$} \\
        \hline
        \hline
        Architecture & Max at & Win rate(P) & Win rate(R) & Max at & Win rate(R) & Win rate(P)  \\ 
        \hline
        Monolithic & 100,000 & 66.0\% & 3.0\% & 159,000 & 98.0\% & 61.20\% \\ 
        \hline
        Seperate Modular & - & - & - & 111,000 & 90.1\% & 28.20\% \\ 
        \hline
        Hybrid Modular & - & - & - & 46,000 & 87.0\% & 17.30\% \\ 
        \hline
    \end{tabular}
\end{table}

When $\lambda=0.7$ and $\alpha=0.01$, for monolithic network the performance did not improve even after 100,000s games and only resulted in a faster convergance for the network; the training wasn't improving the performance. For both modular networks the results quickly diverged for the number of turns per game reaching over 1,000, hence the missing results. When $\lambda$ was closer to 0 and $\alpha=1$, performance significantly improved after the first 1000 games and continued to improve after 100,000 games, figure \ref{mtrain:non}; after 500 games $\alpha$ was modified to 0.1, this was in line to the work of Papahristou and Refanidis \citeyear{DBLP:conf/ifip12/PapahristouR12} as it gave good results. Changing $\alpha$ did make any noticable improvements. so this parameter was kept for the remainder of the experiments and was used for the modular networks training.

For the second stage of experiments, the input features were varied. Table \ref{perf1} shows the performance of the networks with basic input features, with pip count included and with both pip count and hit probability included against pubeval agent after 1000 test games. $\lambda$ and $\alpha$ were not changed. The networks were left to run for as long as possible. The results in the table were for the latest training game reached by each network.
\begin{table}[htb]
    \centering
    \caption{Performance after 1000 test games against Pubeval agent}
    \label{perf1}
    \begin{tabular}{c|cc|cc|cc}
        & \multicolumn{2}{c}{Basic} & \multicolumn{2}{c}{Pip count} & \multicolumn{2}{c}{Pip count + hit probability} \\
        \hline
        \hline
        Architecture & Max at & Win rate & Max at & Win rate & Max at & Win rate \\ 
        \hline
        Monolithic & 159,000 & 61.20\% & 94,000 & 54.00\% & 100,000 & 60.00\% \\ 
        \hline
        Seperate Modular & 111,000 & 28.20\% & 34,000 & 12.20\%  & 12,000 & 20.20\% \\ 
        \hline
        Hybrid Modular & 46,000 & 17.30\% & 78,000 & 36.00\% & 29,000 & 16.80\% \\ 
        \hline
    \end{tabular}
\end{table}

Based on the results of table \ref{perf1}, for the monolithic network, the inclusion of pip count and hit probability features did not improve the performance of the network. The basic features resulted in the best performance against pubeval agent and converged at a much earlier stage, figure \ref{mono:train}. Continuing the training however, resulted in making the performance slightly worst as shown in figure \ref{mtrain:non} after the 190,000 game mark.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-non}.png}
        \subcaption{Basic features}
        \label{mtrain:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-pip}.png}
        \subcaption{Pip count included}
        \label{mtrain:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{mtrain:all}
    \end{minipage}
    \caption{Monolithich network against pubeval agent}
    \label{mono:train}
\end{figure}

For seperate modular network, the inclusion of the extra features resulted in the divergance of the network. A single game would take over 1,000 turns, and the performance was declining, figure \ref{mod:train}. The basic input features gave the best results for this modular network with the trendline increasing at a slow rate reaching a limit.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-non}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-pip}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mod-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{Seperate modular network against pubeval agent}
    \label{mod:train}
\end{figure}

The hybrid modular network benefited from the inclusion of the pip count feature, but it diverged in the other cases, figure \ref{hybrid:train}. No trend for either the basic features and all features instances was observed but the instance with pip count showed an increasing trendline during early training games to reach a limit after the 60,000 game mark.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-non}.png}
        \subcaption{Basic features}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-pip}.png}
        \subcaption{Pip count included}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-all}.png}
        \subcaption{Pip count and hit probability included}
        \label{plot:all}
    \end{minipage}
    \caption{Hybrid modular network against pubeval agent}
    \label{hybrid:train}
\end{figure}

\subsection{Match analysis}
The best performing networks were tested against GNU-Backgammon's world class level agent for 10 games. The best performing network were: the monolithic network with basic input features, the seperate modular network with basic input features and the hybrid modular network with pip count included. 
\subsubsection{Monolithic Network}
Monolithic network made bad decisions in some positions but generally picked one of the 3 top moves during the game. The monolithic network tended to use primes as part of its strategy; this was observed by watching the network play against Pubeval and against GNU-Backgammon agent. As the game reached the racing stage, the network made average decisions by picking moves that were in the middle range, 17th move picked out of 25 possible moves, based on GNU-Backgammon analysis, figure \ref{mana}. The monolithic network won 1 out 10 games against GNU-Backgammon agent.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-racing-board}.png}
        \subcaption{Game Board}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-racing-rollout}.png}
        \subcaption{Move analysis}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{mono-feedback}.png}
        \subcaption{Full game analysis}
        \label{mana-full}
    \end{minipage}
    \caption{Monolithic network (Red) against GNU-Backgammon world class level agent (Black) - racing stage}
    \label{mana}
\end{figure}

\subsubsection{Seperate Modular Network}
Seperate modular network played good first roll positions, made many top decisions at the racing stage. It occasionally picked one of 3 top moves during the game but made more bad decisions than the monolithic, figure \ref{hana}. The hybrid network tended to pick moves that reduced the pip count which highlighted the influence of including the pip count to the features. In terms of strategy, the hybrid network seemed to build primes but also kept 2 checkers at the 13th field for the majority of the game. It didn't follow any resemblance to a back game strategy. The hybrid modular network won 1 out 10 games against GNU-Backgammon agent.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-board}.png}
        \subcaption{Game Board}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-move}.png}
        \subcaption{Move analysis}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-moves}.png}
        \subcaption{Full game analysis}
        \label{mana-full}
    \end{minipage}
    \caption{Hybrid modular network (Red) against GNU-Backgammon world class level agent (Black)}
    \label{hana}
\end{figure}

\subsubsection{Hybrid Modular Network}
Hybrid modular network played good first roll positions, made many top decisions during the racing stage of the game. It occasionally picked one of 3 top moves during the game but made more bad decisions than the monolithic network, figure \ref{hana}. The hybrid network tended to pick moves that reduced the pip count which proved the influence of including the pip count to the features. In terms of strategy, the hybrid network tended to build primes but also kept 2 checkers at the 13th field for the majority of the game. It didn't follow any resemblance to a back game strategy. The hybrid modular network won 1 out 10 games against GNU-Backgammon agent.
\begin{figure}[htp] 
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-board}.png}
        \subcaption{Game Board}
        \label{plot:non}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-move}.png}
        \subcaption{Move analysis}
        \label{plot:all}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{{hybrid-bad-moves}.png}
        \subcaption{Full game analysis}
        \label{mana-full}
    \end{minipage}
    \caption{Hybrid modular network (Red) against GNU-Backgammon world class level agent (Black)}
    \label{hana}
\end{figure}


\section{Evaluation}
In this section, an evaluation of the solution is given based on the results obtained. The solution strengths and limitations along with potential improvements are discussed.
\subsection{Solution Strength and limitations}
Changing the training parameters, $\lambda$ in particular, strongly influenced the general performance of the networks as evident from the results in table \ref{rp}. Reducing $\lambda$ greatly improved the performance of the networks. The inclusion of more input features did not provide any noticable improvement to the monolithic network performance against the benchmark agent, pubeval. However, it caused instability to the seperate modular network's training. Convergences was not gauranteed and different runs for the same configurations could either diverge or converge. This made training incredibly unstable with various results obtained in the various runs. A better training strategy would be promising in solving this problem. Similar issue could be observed with the hybrid modular network results, figure \ref{hybrid:train}. Although, the inclusion of pip count improved the performance, for the other two cases the results diverged. 

The general performance and game analysis of the monolithic network was within the results obtained by Tesauro \citeyear{DBLP:journals/ai/Tesauro02} and Papahristou and Refanidis \citeyear{DBLP:conf/ifip12/PapahristouR12}. On the other hand, for modular networks, as the architecture of the networks was based on the architecture's description in Boyan's \citeyear{boyan} and Wiering's \citeyear{DBLP:journals/jilsa/Wiering10} work along with notes included in GNU-Backgammon software \cite{gnubg}, the actual implementation did not match their results Overall, the results proved that the monolithic network outperformed both modular networks. This result contradicted the expected state-of-art results that the modular networks would perform better than the monolithic network; GNU-Backgammon agents are ranked as one of the best AI agents for backgammon \cite{exg}. In terms of the best modular architecture, the best hybrid modular network performed better than the best seperate modular network against the benchmark agent, pubeval. 

The results obtained from GNU-Backgammon software provided some insight into the strategy used by all networks. The priming strategy seemed to be the dominant strategy followed by all networks. The analysis emphasised the weakness of the monolithic network during the racing stage as noted in Frnkranz 's survey \citeyear{survey}. The hybrid network provided a better strategy for the racing stage than the monolithic network but it did not provide a better strategy for the other positions. 

Answering the question wether the hybrid modular network had resulted in a new strategy remains a subjective matter. The hybrid network did learn a different strategy, however as evident from the results in table \ref{perf1} and the analysis in figure \ref{hana}, it was clear that the strategy was decent but not great. 

\subsection{Improvements}
If given a chance to repeat this project, the advance objectives would have been explored and the training strategy for the modular networks would be re-evaluated to solve the observed divergence issue. As most bugs in the code has been fixed and a better performance measure had been included by using pubeval and GNU-Backgammon software, more parameters could be experimented. Doubling cube would be included as part of the actions selection algorithm. In addition, the action selection algorithm would be extended to include more lookahead, performing a 2-ply or a 3-ply search which had been proven to provide better performances as evident from results collected by Depreli \citeyear{botbattle}. Various online resources claimed short training time even for 200,000 games, so different packages that would be more optimized to use the CPU would be considered to reimplement this project and reduce the training time.

\section{Conclusions}
The implemented solution had it flows but showed some promise. Minor tweeks to the networks parameters resulted in better performance against the benchmark agent, pubeval, as indicated by the experiments with the learning parameters $\lambda$ and $\alpha$ along with extracting additional features from the game state and including them to the network's input units. Long training time limited the number of parameters expiremented, but decent results were obtained. Modular network architecture had improved play at some stages of the game and resulted in different strategies than the monolithic network architecture, however, it did not perform better in terms of the overall game strategy. This was highlighted by the match analysis for the networks against GNU-Backgammon world class level agent. Further, improvements to the modular network could be made espicially to the activation algorithm for the sub-networks, it could be either by redefining the conditions or incorporating the Meta-pi architecture; the addition of a network to handle the networks activations. Other combintations of backgammon strategies could result in interesting new strategies that would be better than the combination of the back and priming games studied in this project. The priming game strategy was generally more dominant in the hybrid modular network as observed from the test games and match analysis. The hybrid modular network performed better than the modular network with seperate strategies. Carrying this project forward, more complex search algorithms for action selection could be incorporated along with the decision algorithm for using doubling cude during the game. These algorithm could produce better strategies at the expense of increasing the complexity of the implemeneted solution. 


\bibliography{projectpaper}


\end{document}